[
  {
    "objectID": "Mauricio-CV.html",
    "href": "Mauricio-CV.html",
    "title": "CV",
    "section": "",
    "text": "Researcher with 9+ years of experience in the development of mathematical models (ranging from differential equations to machine-learning models) in the fields of environmental biotechnology and public health epidemiology. Passionate about creating models that inform decision-making and address real-world challenges."
  },
  {
    "objectID": "Mauricio-CV.html#postdoctoral-researcher-at-khalifa-university--today",
    "href": "Mauricio-CV.html#postdoctoral-researcher-at-khalifa-university--today",
    "title": "CV",
    "section": "Postdoctoral Researcher at Khalifa University -today",
    "text": "Postdoctoral Researcher at Khalifa University -today\n\nCOVID-19 Policy Insights:\n\nDeveloped custom models for the Department of Health of Abu Dhabi that were used for policy decision-making during the pandemic, including:\n\nAn online simulation tool (deployed in R and Shiny) to simulate the spread of COVID-19 and assess the effectiveness of potential interventions.\nA model that evaluated vaccine distribution strategies across different age groups to minimize fatalities.\n\nSimulated for the Expo 2020 organizers the impact of international visitors on large-scale events to gain insights on how to balance infection rates and economic impact.\nPerformed ETL on COVID-19 data sourced from the DOH. This involved curating and consolidating data, and automating the process to ensure accurate case reporting and estimate model parameters.\nDelivered to DOH stakeholders periodic automated COVID-19 risk assessment reports using R and RMarkdown (i.e. evaluation of the hospitalization risk associated with different SARS-CoV-2 variants).\n\n\n\nSecured University Funding:\n\nSuccessfully developed and implemented research proposals worth 1.3 million AED in funding from the Department of Health (DOH) to Khalifa University.\n\n\n\nWastewater Treatment Modelling:\n\nDeveloped a customized online simulation dashboard for ENEC using Python to evaluate the effectiveness and costs (CAPEX/OPEX) of the proposed wastewater treatment system."
  },
  {
    "objectID": "Mauricio-CV.html#phd-candidate-at-masdar-institute---khalifa-university--19",
    "href": "Mauricio-CV.html#phd-candidate-at-masdar-institute---khalifa-university--19",
    "title": "CV",
    "section": "PhD Candidate at Masdar Institute - Khalifa University -19",
    "text": "PhD Candidate at Masdar Institute - Khalifa University -19\n\nSystematic Methodology for the Evaluation of Metabolic Pathways:\n\nDeveloped a novel method to compute energy yields of all combinations for specific metabolic pathways. This can be used to inform the design of bioprocess systems (i.e. set conditions to increase a product yield).\n\n\n\nModel Improvement in Bioreactor Simulations:\n\nEnhanced an existing software in MATLAB and in Python to integrate the dynamic calculation of the bioenergetics of microbial reactions, resulting in an estimated 5% improvement of model predictions."
  },
  {
    "objectID": "Mauricio-CV.html#intern-bioprocess-engineer-at-paques-bv-the-netherlands",
    "href": "Mauricio-CV.html#intern-bioprocess-engineer-at-paques-bv-the-netherlands",
    "title": "CV",
    "section": "Intern Bioprocess Engineer at Paques BV (The Netherlands) ",
    "text": "Intern Bioprocess Engineer at Paques BV (The Netherlands) \n\nOperated a small-scale bioreactor and conducted lab sample measurements for a wastewater treatment project at a bioethanol production plant from St1 Nordic (Finland). This work eventually resulted in the successful installation of a €5 million bioreactor at the client’s facility in 2015."
  },
  {
    "objectID": "mauricio-quarto-blog.html",
    "href": "mauricio-quarto-blog.html",
    "title": "Portfolio Mauricio",
    "section": "",
    "text": "I am Mauricio Paton and this is my blog. I am going to aggregate here all my projects and also interests."
  },
  {
    "objectID": "mauricio-quarto-blog.html#hello-there",
    "href": "mauricio-quarto-blog.html#hello-there",
    "title": "Portfolio Mauricio",
    "section": "",
    "text": "I am Mauricio Paton and this is my blog. I am going to aggregate here all my projects and also interests."
  },
  {
    "objectID": "Mauricio-CV-DS.html",
    "href": "Mauricio-CV-DS.html",
    "title": "CV",
    "section": "",
    "text": "I am a researcher with over 9 years of expertise in mathematical modeling. My work covers diverse fields, from the development of models for environmental biotechnology and also public health epidemiology. I have demonstrated ability to quickly adapt and excel across various domains.\nCore competencies include:\nSoftware Skills: Proficient in Python, R, and MATLAB for data analysis, machine learning, and scientific computing. Experience with Git, unit testing, and code formatting to ensure code quality and collaboration. *Soft Skills:** Research proficiency, proposal writing, effective presentation organization."
  },
  {
    "objectID": "Mauricio-CV-DS.html#postdoctoral-researcher-at-khalifa-university--today",
    "href": "Mauricio-CV-DS.html#postdoctoral-researcher-at-khalifa-university--today",
    "title": "CV",
    "section": "Postdoctoral Researcher at Khalifa University -today",
    "text": "Postdoctoral Researcher at Khalifa University -today\n\nMathematical Modelling for COVID-19:\n\nDeveloped compartmental models to assess the spread of COVID-19.\nWrote a web scraper to download automatically data published by the Ministry of Health in Spain to infer epidemiological parameters.\nEvaluated the effectiveness of potential interventions. including vaccine distribution strategies across different age groups to minimize fatalities.\n\n\n\nCOVID-19 Policy Insights:\n\nCollaborated with the Department of Health (DOH) in Abu Dhabi.\nSimulated pandemic trajectories to inform decision-makers about effective interventions while considering economic impact.\nEvaluated hospitalization risks associated with different SARS-CoV-2 variants.\nEnsured accurate alignment of COVID-19 cases and hospitalization data through record merging, and validation.\n\n\n\nSecured University Funding:\n\nPlayed a pivotal role in securing 1.3 million AED in funding for Khalifa University.\nDeveloped and executed research proposals that facilitated project agreements between the DOH and Khalifa University.\n\n\n\nWastewater Treatment Modelling:\n\nDeveloped an online simulation dashboard to showcase the efficiency of the proposed wastewater treatment for an industrial wastewater reach in ethanolamine."
  },
  {
    "objectID": "Mauricio-CV-DS.html#phd-candidate-at-masdar-institute-khalifa-university--19",
    "href": "Mauricio-CV-DS.html#phd-candidate-at-masdar-institute-khalifa-university--19",
    "title": "CV",
    "section": "PhD Candidate at Masdar Institute / Khalifa University -19",
    "text": "PhD Candidate at Masdar Institute / Khalifa University -19\n\nSystematic Methodology for Bioenergetics Evaluation of Metabolic Pathways:\n\nDeveloped a systematic approach to assess the ATP yield generated in all possible combinations of pathway variants in a metabolic pathway.\nApplied this methodology to evaluate the optimal ATP yield in the oxidation of propionic acid in anaerobic conditions.\nProvided valuable insights into which pathways yield most energy and under which physiological an environmental conditions.\n\n\n\nBioenergetics Integration in a Bioreactor Simulator:\n\nEnhanced an existing bioreactor software simulator by integrating a generalized approach for the dynamic calculation of the bioenergetics of microbial reactions.\n\nApplied the bioreactor simulator to evaluate the impact of bioenergetics on kinetic processes under anaerobic conditions."
  },
  {
    "objectID": "posts/bioreactor-dashboard/index.html",
    "href": "posts/bioreactor-dashboard/index.html",
    "title": "Bioreactor online simulator dashboard",
    "section": "",
    "text": "An online dashboard developed using Python and Shiny for Python was developed. This dashboard consists of a web simulator that shows the effectiveness of the proposed treatment for a wastewater rich in ethanolamine.\nThe dashboard shows the effluent from each unit proposed in the treatment train, and also includes an estimation of the capital and operational expenses that would be incurred for treating such wastewater."
  },
  {
    "objectID": "posts/bioreactor-dashboard/index.html#dashboard-description",
    "href": "posts/bioreactor-dashboard/index.html#dashboard-description",
    "title": "Bioreactor online simulator dashboard",
    "section": "Dashboard description",
    "text": "Dashboard description\nThis dashboard is a demonstration tool on how a train unit consisting of a UASB (Upflow Anaerobic Slugde Blanket) together with a partial nitrification/anammox system would work to remove ethanolamine from the wastewater. The dashboard includes also an estimation of the potential costs that would be incurred for treating such wastewater following the design proposed.\nThe adjustable parameters to evaluate the treatment proposed are as follows:\n\nOperational Parameters\n\nVolumes of the reactors (in L):\n\nUASB\nPartial nitrification\nAnammox\n\nConcentration of oxygen of the partial nitrification reactor (in mg/L)\n\n\n\nInfluent concentrations:\n\nEthanolamine\n\nThe user can define on the left handside the values for each one of the parameters defined and the model will run a simulation to reach steady state. The figure will then plot the concentrations in the influent and in the effluent of each unit proposed in the treatment train."
  },
  {
    "objectID": "posts/bioreactor-dashboard/index.html#technical-features",
    "href": "posts/bioreactor-dashboard/index.html#technical-features",
    "title": "Bioreactor online simulator dashboard",
    "section": "Technical features",
    "text": "Technical features\nThis code was developed in Python and the dashboard was created using Shiny for Python.\nFirstly, the code containing the bioreactor model is wrote in a modular way and can be executed by running the run_simulation.py file.\nThe code is then executed using this workflow\n\n\n\n\ngraph LR\nA[run_simulation.py] --&gt; B[bioreactor_model.py]\nA --&gt; C[bioreactor_cost.py]\nB --&gt; D[bioreactor_parameters.py]\nC --&gt; D\n\n\n\n\n\n\n\nThe online simulation tool is available in this website. An snapshot of the model simulator is shown in Figure 1.\n\n\n\nFigure 1: Template of the dashboard including the online model simulator for a series of bioreactors\n\n\nThe main figure shows the concentrations of the compounds of interest in steady state in the effluent of each one of the bioreactor units considered (e.g. UASB, Partial nitrification and anammox).\nAn estimation of the capital expenditure based on the volumes of each reactor unit is provided, together with an estimation of the operational expenses based on the size of each bioreactor and the oxygen requirements for the partial nitrification reactor."
  },
  {
    "objectID": "posts/bioreactor-dashboard/index.html#further-improvements",
    "href": "posts/bioreactor-dashboard/index.html#further-improvements",
    "title": "Bioreactor online simulator dashboard",
    "section": "Further improvements",
    "text": "Further improvements\nFurther improvements to this dashboard include (but not limited to):\n\nCalculate volumes based on discharge limits in steady state.\nInclude dynamic concentrations results for each bioreactor.\nRefine economic calculations (CAPEX and OPEX)."
  },
  {
    "objectID": "posts/vaccine-allocation-simulation/index.html",
    "href": "posts/vaccine-allocation-simulation/index.html",
    "title": "COVID-19 Vaccine allocator",
    "section": "",
    "text": "This is a post to explain what was done to simulate the allocation of vaccines during the COVID-19 pandemic.\nThis code and its figures were developed in MATLAB.\n\n\n\nFigure 1: Vaccine allocation simulation for different age groups\n\n\nThe model simulating the allocation of vaccined as per age groups can be seen in Figure 1.\nGet some information from here and here"
  },
  {
    "objectID": "posts/covid19-impact-expo20/index.html",
    "href": "posts/covid19-impact-expo20/index.html",
    "title": "COVID-19 Expo Model",
    "section": "",
    "text": "This is a post to explain how a simulation conducted to evaluate the impact of infected visitors to international large events was conducted.\nThis work was highlighted in local media showcasing the impact of modelling in the decision-making for Expo2020 in [this article] and [this article]. In addition, a MoU between Department of Health of Abu Dhabi and Khalifa University was signed at Expo.\n\n\n\nFigure 1: Template of the online model simulator for COVID-19\n\n\nThe model simulator can be seen in Figure 1."
  },
  {
    "objectID": "posts/icr-kaggle/index.html",
    "href": "posts/icr-kaggle/index.html",
    "title": "Identify age-related conditions Competition",
    "section": "",
    "text": "This was the notebook that I used to submit my first competition in kaggle. The competition, hosted by ICR, focused on the identification of Age-Related conditions. Kaggle provided an example implementation using Tensorflow Decision Forests.\n\n\n\nimage\n\n\n\nGoal of the Competition:  The aim of this competition is to develop a predictive model for three specific medical conditions. Participants are tasked with predicting whether an individual has one or more of these conditions (Class 1), or none of them (Class 0). The model should be trained using health measurement data."
  },
  {
    "objectID": "posts/icr-kaggle/index.html#introduction",
    "href": "posts/icr-kaggle/index.html#introduction",
    "title": "Identify age-related conditions Competition",
    "section": "",
    "text": "This was the notebook that I used to submit my first competition in kaggle. The competition, hosted by ICR, focused on the identification of Age-Related conditions. Kaggle provided an example implementation using Tensorflow Decision Forests.\n\n\n\nimage\n\n\n\nGoal of the Competition:  The aim of this competition is to develop a predictive model for three specific medical conditions. Participants are tasked with predicting whether an individual has one or more of these conditions (Class 1), or none of them (Class 0). The model should be trained using health measurement data."
  },
  {
    "objectID": "posts/icr-kaggle/index.html#load-training-set",
    "href": "posts/icr-kaggle/index.html#load-training-set",
    "title": "Identify age-related conditions Competition",
    "section": "Load training set",
    "text": "Load training set\nInitially, we will proceed with loading the data. For this competition, a train.csv and a greeks.csv file are provided. The test set, available online via Kaggle, was utilized for competing on the public leaderboard.\nOriginally, this notebook was executed on Kaggle, hence the folder paths were set according to Kaggle’s server guidelines. These paths have since been adjusted to enable local execution of the code.\n\n\nCode\nimport pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\nTRAIN_SET_FILE = \"./data/train.csv\"\nINDEX_COLUMN = 'Id'\n\ndf_icr = pd.read_csv(TRAIN_SET_FILE, index_col = INDEX_COLUMN)\ndf_icr.shape\n\n\n(617, 57)\n\n\nWe see that we have a rather small dataset (617 rows) with a large number of columns (57). We can examine the initial rows of the training set to gain a preliminary understanding of the dataset.\n\n\nCode\ndf_icr.head(10)\n\n\n\n\n\n\n\n\n\nAB\nAF\nAH\nAM\nAR\nAX\nAY\nAZ\nBC\nBD\n...\nFL\nFR\nFS\nGB\nGE\nGF\nGH\nGI\nGL\nClass\n\n\nId\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n000ff2bfdfe9\n0.209377\n3109.03329\n85.200147\n22.394407\n8.138688\n0.699861\n0.025578\n9.812214\n5.555634\n4126.58731\n...\n7.298162\n1.73855\n0.094822\n11.339138\n72.611063\n2003.810319\n22.136229\n69.834944\n0.120343\n1\n\n\n007255e47698\n0.145282\n978.76416\n85.200147\n36.968889\n8.138688\n3.632190\n0.025578\n13.517790\n1.229900\n5496.92824\n...\n0.173229\n0.49706\n0.568932\n9.292698\n72.611063\n27981.562750\n29.135430\n32.131996\n21.978000\n0\n\n\n013f2bd269f5\n0.470030\n2635.10654\n85.200147\n32.360553\n8.138688\n6.732840\n0.025578\n12.824570\n1.229900\n5135.78024\n...\n7.709560\n0.97556\n1.198821\n37.077772\n88.609437\n13676.957810\n28.022851\n35.192676\n0.196941\n0\n\n\n043ac50845d5\n0.252107\n3819.65177\n120.201618\n77.112203\n8.138688\n3.685344\n0.025578\n11.053708\n1.229900\n4169.67738\n...\n6.122162\n0.49706\n0.284466\n18.529584\n82.416803\n2094.262452\n39.948656\n90.493248\n0.155829\n0\n\n\n044fb8a146ec\n0.380297\n3733.04844\n85.200147\n14.103738\n8.138688\n3.942255\n0.054810\n3.396778\n102.151980\n5728.73412\n...\n8.153058\n48.50134\n0.121914\n16.408728\n146.109943\n8524.370502\n45.381316\n36.262628\n0.096614\n1\n\n\n04517a3c90bd\n0.209377\n2615.81430\n85.200147\n8.541526\n8.138688\n4.013127\n0.025578\n12.547282\n1.229900\n5237.54088\n...\n0.173229\n0.49706\n1.164956\n21.915512\n72.611063\n24177.595550\n28.525186\n82.527764\n21.978000\n0\n\n\n049232ca8356\n0.348249\n1733.65412\n85.200147\n8.377385\n15.312480\n1.913544\n0.025578\n6.547778\n1.229900\n5710.46099\n...\n4.408484\n0.86130\n0.467337\n17.878444\n192.453107\n3332.467494\n34.166222\n100.086808\n0.065096\n0\n\n\n057287f2da6d\n0.269199\n966.45483\n85.200147\n21.174189\n8.138688\n4.987617\n0.025578\n9.408886\n1.229900\n5040.77914\n...\n6.591896\n0.49706\n0.277693\n18.445866\n109.693986\n21371.759850\n35.208102\n31.424696\n0.092873\n0\n\n\n0594b00fb30a\n0.346113\n3238.43674\n85.200147\n28.888816\n8.138688\n4.021986\n0.025578\n8.243016\n3.626448\n6569.37001\n...\n4.762291\n1.18262\n0.067730\n17.245908\n147.218610\n4589.611956\n29.771721\n54.675576\n0.073416\n0\n\n\n05f2bc0155cd\n0.324748\n5188.68207\n85.200147\n12.968687\n8.138688\n4.593392\n0.025578\n10.685041\n1.229900\n4951.69863\n...\n0.173229\n1.57151\n0.318331\n24.515421\n98.929757\n5563.130949\n21.994831\n33.300970\n21.978000\n0\n\n\n\n\n10 rows × 57 columns\n\n\n\nWe can see from the table that there are several columns, each representing a distinct measured feature. These features however are obfuscated, making it really difficult and sometimes impossible to determine which column corresponds to which actual measurement. Additionally, it’s noticeable that the values for each feature span various ranges.\nAs stated in the competition goal, our task is to utilize the information from these features to predict whether a patient has any of the three age-related conditions (Class 1) or none of these conditions (Class 0)."
  },
  {
    "objectID": "posts/icr-kaggle/index.html#data-preprocessing",
    "href": "posts/icr-kaggle/index.html#data-preprocessing",
    "title": "Identify age-related conditions Competition",
    "section": "Data preprocessing",
    "text": "Data preprocessing\nLooking at the features from the train data, their mean and standard deviation appear to be very different in terms of range. Therefore, a good approach for using the different models later on will be to standardize or normalize all variables. Models work better with scaled variables as the optimization becomes more stable numerically. In fact, we might try both methods to see if they work.\n\n\nCode\n# Identify nan values in columns\nid_nan_cols = df_icr.columns[df_icr.isna().sum() &gt; 0]\nprint(\"There are nan values in the following columns: \")\ndf_icr[id_nan_cols].isna().sum()\n\n\nThere are nan values in the following columns: \n\n\nBQ    60\nCB     2\nCC     3\nDU     1\nEL    60\nFC     1\nFL     1\nFS     2\nGL     1\ndtype: int64\n\n\nWe can see that there are 9 columns containing null values, from which only BQ and EL are the ones with most nan values (~10% of those).\nWe will transform the data by using a pipeline in which we will input the average value and then we will scale all the values between 0 and 1. By writing the pipeline, we should be able to apply the same for the test set.\nWe should also plot the correlation between variables, since many age related conditions might present similar features. In addition, we should plot the features as boxplots to check for outliers.\nLastly, we need to check for the class output, to check if the dataset is imbalanced or not. .\n\n\nCode\n\nfeatures = df_icr.columns.drop(target_variable).values\nfeatures\n\nX_train = df_icr[features]\ny_train = df_icr[target_variable]\n\n\nNameError: name 'target_variable' is not defined"
  },
  {
    "objectID": "posts/icr-kaggle/index.html#eda",
    "href": "posts/icr-kaggle/index.html#eda",
    "title": "Identify age-related conditions Competition",
    "section": "EDA",
    "text": "EDA\nIn this section, we will be conducting a visual exploration of data distributions, correlations and potential trends. We will also investigate whether there is an imbalance in the classification class and quantify the number of missing values present in the dataset.\n\nDistribution of target variable\nLet’s examine whether the target variable Class is balanced or not:\n\n\nCode\nY_VARIABLE = 'Class'\n\nsns.countplot(x = df_icr[Y_VARIABLE])\nsns.set_style(\"darkgrid\")\n\n\n\n\n\nObserving the data, we notice an imbalance with a ratio of 1:5. In the context of medical conditions, this is typical, as there are usually more individuals without the condition than those affected. It’s important to consider this during model training—whether to penalize incorrect predictions or explore the impact of balanced training data on performance.\n\n\nIdentification of null-values\nIn addition, there are some variables that have null-values. We need to check how are those null-values for tsome variables to decide whether imputation of dropping entirely rows or columns is a better approach. In the next cell, we will identify null-values per feature.\n\n\nCode\n# Identify nan values in columns\nid_nan_cols = df_icr.columns[df_icr.isna().sum() &gt; 0]\nprint(\"There are nan values in the following columns: \")\ndf_icr[id_nan_cols].isna().sum()\n\n\nThere are nan values in the following columns: \n\n\nBQ    60\nCB     2\nCC     3\nDU     1\nEL    60\nFC     1\nFL     1\nFS     2\nGL     1\ndtype: int64\n\n\nWe can see that there are 9 columns containing null values, from which only BQ and EL are the ones with most nan values (~10% of those).\nWe will transform the data by using a pipeline in which we will input the average value and then we will scale all the values between 0 and 1. By writing the pipeline, we should be able to apply the same for the test set.\nWe should also plot the correlation between variables, since many age related conditions might present similar features. In addition, we should plot the features as boxplots to check for outliers.\nLastly, we need to check for the class output, to check if the dataset is imbalanced or not.\n\n\nCode\nfeatures = df_icr.columns.drop(Y_VARIABLE).values\nprint(features)\n\nX_train = df_icr[features]\ny_train = df_icr[Y_VARIABLE]\n\n\n['AB' 'AF' 'AH' 'AM' 'AR' 'AX' 'AY' 'AZ' 'BC' 'BD ' 'BN' 'BP' 'BQ' 'BR'\n 'BZ' 'CB' 'CC' 'CD ' 'CF' 'CH' 'CL' 'CR' 'CS' 'CU' 'CW ' 'DA' 'DE' 'DF'\n 'DH' 'DI' 'DL' 'DN' 'DU' 'DV' 'DY' 'EB' 'EE' 'EG' 'EH' 'EJ' 'EL' 'EP'\n 'EU' 'FC' 'FD ' 'FE' 'FI' 'FL' 'FR' 'FS' 'GB' 'GE' 'GF' 'GH' 'GI' 'GL']\n\n\n\n\nCorrelations between features\nNow it is time to explore how the features are distributed. We can do so through the following visualizations:\n\nCorrelation plot\nBoxplots\nHistograms\n\n\n\nCode\n# Define correlation threshold\ncorr_threshold = 0\n\n# Identify correlation in data\ncorrelation_features = df_icr.corr()\ncorrelation_signal = correlation_features[np.abs(correlation_features) &gt; corr_threshold]\ncorrelation_signal = correlation_signal.fillna(0)\n\nlow_correlation_columns =  correlation_signal.index[correlation_signal.sum(axis= 1) == 1]\nsns.heatmap(correlation_features[np.abs(correlation_features) &gt; corr_threshold])\n\n\n/tmp/ipykernel_49989/298676056.py:5: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n  correlation_features = df_icr.corr()\n\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\nCode\nsns.set_theme()\nsns.set(rc={\"figure.dpi\":300, 'savefig.dpi':300})\nsignal_correlation_columns = correlation_signal.index[correlation_signal.sum(axis= 1) &gt; 1]\nprint(signal_correlation_columns)\nnum_signal_columns = len(signal_correlation_columns)\nnum_signal_columns \n\n\nIndex(['AB', 'AF', 'AH', 'AM', 'AR', 'AX', 'AY', 'AZ', 'BC', 'BD ', 'BN', 'BP',\n       'BQ', 'BR', 'BZ', 'CB', 'CC', 'CD ', 'CF', 'CH', 'CL', 'CS', 'CU',\n       'CW ', 'DA', 'DE', 'DF', 'DH', 'DI', 'DL', 'DN', 'DU', 'DV', 'EB', 'EE',\n       'EG', 'EH', 'EL', 'EP', 'EU', 'FC', 'FD ', 'FE', 'FI', 'FL', 'FS', 'GB',\n       'GE', 'GF', 'GH', 'GI', 'GL', 'Class'],\n      dtype='object')\n\n\n53\n\n\nTo get an initial understanding of the features of the dataset, we can start by visualizing the distribution of the features. We can do so by running a histogram of all the numerical variables to see whether variables ar econtinuous or discrete. We will select 20 bins for the histograms.\n\n\nCode\nNUM_PLOT_COLUMNS = 5\nUSE_HISTOGRAM = True \n\nplt.figure().clear()\n\nfig, axes = plt.subplots(int(num_signal_columns/NUM_PLOT_COLUMNS), NUM_PLOT_COLUMNS, figsize=(15, 30), sharey=False)\nfig.suptitle('Distibution of continuous variables')\nfig.tight_layout() \n\nidrow = -1\nfor idx, col in enumerate(signal_correlation_columns):\n\n    idcol = int(np.mod(idx, NUM_PLOT_COLUMNS))\n    idrow = idrow + 1 if idcol == 0 else idrow\n    \n    if USE_HISTOGRAM:\n        sns.histplot(ax=axes[idrow, idcol], data = df_icr, x = col, bins = 20)\n    else:\n        sns.boxplot(ax=axes[idrow, idcol], data = df_icr, x = col)\n\n    axes[idrow, idcol].set_xlim(0, np.max(df_icr[col]) * 1.1)\n\n\nIndexError: index 10 is out of bounds for axis 0 with size 10\n\n\n&lt;Figure size 1920x1440 with 0 Axes&gt;\n\n\n\n\n\nAs we can see, we have some outliers on some variables, and others appear to be discrete rather than continuous. We are going to define which of those are numerical initially before doing any extra modification on the data.\nLetś first chekck for outliers doing a boxplot\n\n\nCode\n# Checking for Outliers\nfig, ax = plt.subplots(4, 5, figsize=(16,30))\n\nfor idx, col in enumerate(signal_correlation_columns):\n    sns.boxplot(x=\"Class\",y=col,data=df_icr, ax=ax[int(idx/5), idx % 5])\n\n# Adjust the vertical spacing between subplots    \nplt.tight_layout()\n\nplt.show()\n\n\n\n\n\n\n\nCode\n# continuous_features = ['AF', ]\n# discrete_features = ['AB', 'AH', 'AM', 'BZ', 'EU', 'GL']\n\n# # num_plot_columns = 3\n\n# # plt.figure().clear()\n# # fig, axes = plt.subplots(int(len(discrete_features)/num_plot_columns), \n# #                          num_plot_columns, figsize=(15, 5), sharey=True)\n# # fig.suptitle('Histogram of discrete variables')\n# # fig.tight_layout() # Or equivalently,  \"plt.tight_layout()\"\n\n# # idrow = -1\n# # for idx, col in enumerate(discrete_features):\n# #     idcol = int(np.mod(idx, num_plot_columns))\n# #     if idcol == 0:\n# #         idrow += 1  \n# #     sns.histplot(ax=axes[idrow, idcol], data = df_icr, x = col, bins = 20)\n# #     # sns.countplot(ax=axes[idrow, idcol], data = df_icr, x = col)\n\n# #     # axes[idrow, idcol].set_title(col)\n\n# fig, ax = plt.figure()\n\n# sns.histplot(data = df_icr, x = col, bins = 50)\n# ax.set_xlim(0, 5)"
  },
  {
    "objectID": "posts/icr-kaggle/index.html#data-preprocessing-1",
    "href": "posts/icr-kaggle/index.html#data-preprocessing-1",
    "title": "Identify age-related conditions Competition",
    "section": "Data preprocessing",
    "text": "Data preprocessing\nWe are going to use a scikit-learn Pipeline to structure our preprocessing. The reason to use a Pipeline is because we will need to conduct the same transformations conducted in the train set into the test set. By storing them in a Pipeline, it is easier to retrieve the transformation when the test set is provided. In the context of machine learning in production, it is saved to conduct the same transformations when new data arrives.\nThe preprocessing is going to follow these steps:\n\nImputation of missing values with KNNImputer. The reason for using KNNimputer is to avoid changing the underlying distribution of the feature. Alternative imputation methods involve using the mean for the values imputed, but that decreases the variance of the feature and might distort the relationship between variables.\nScale columns from 0-1 using MixMaxScaler. This should help the model to converge faster and potentially offering better results.\nChange categorical columns into ordinal numbers.\n\n\n\nCode\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OrdinalEncoder, OneHotEncoder\nfrom sklearn.impute import SimpleImputer, KNNImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn import set_config\nset_config(transform_output = \"pandas\")\n\nnumerical_features   = features[X_train.dtypes == \"float\"]\ncategorical_features = features[X_train.dtypes == \"object\"]\n\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', KNNImputer()), \n    ('scaler', MinMaxScaler()),\n    ])\n    \n\ncategorical_transformer = Pipeline(steps=[\n    ('categorizer', OrdinalEncoder()),\n    ])\n\npreprocessor = ColumnTransformer(transformers=[\n    ('num', numeric_transformer, numerical_features),\n    ('cat', categorical_transformer, categorical_features),\n])\n\ndata_pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n\nX_train_transformed = data_pipeline.fit_transform(X_train)\nX_train_transformed\n\n\nTypeError: set_config() got an unexpected keyword argument 'transform_output'\n\n\nThe preprocessing has been conducted in the train set. We should check that the missing values have been addressed. We do this in the next cell.\n\n\nCode\ntotal_nans = X_train_transformed.isna().sum().sum()\nprint(f\"There are {total_nans} nans in the transformed dataset.\")\n\n\nWe should also examine that the scaling has worked properly by taking al ook at the first rows of the X_train_transformed variable\n\n\nCode\nX_train_transformed.head(10)\n\n\n\n\nCode\nX_train.head(10)\n\n\nCheck the balance of the records to make sure that the evaluation of the ML model takes the weights into account\n\n\nCode\npositive_records = y_train.sum()\nnegative_records = (y_train == 0).sum()\npositive_records, negative_records\n# X_test_transformed = data_pipeline.fit_transform(X_test)\n# X_test_transformed.head()\npositive_weight = negative_records / positive_records\nprint('y Estimate: %.3f' % positive_weight)"
  },
  {
    "objectID": "posts/icr-kaggle/index.html#split-the-data",
    "href": "posts/icr-kaggle/index.html#split-the-data",
    "title": "Identify age-related conditions Competition",
    "section": "Split the data",
    "text": "Split the data\nSince we only have a train set and we do not really have a test set, we need to ‘create’ an evaluation set for the data we are going to be training.\nIn addition, we need to ensure that the train and validation sets are split correctly regargding the output variable. Otherwise, we could end up in a Class distribution of 7:1 ratio for the train set and 3:2 for the validation set. This would cause most likely overfitting as the model could predict well the train set but not well on the validation set.\n\n\nCode\n# Leave an out of fold sample\nfrom sklearn.model_selection import train_test_split\n\nseed = 42\nvalidation_size = 0.15\nX_train, X_validation, y_train, y_validation = train_test_split(X_train_transformed, y_train, \n                                                                test_size = validation_size,\n                                                                random_state = seed, stratify=y_train)\n\nec1_eval_set = [(X_train, y_train), (X_validation, y_validation)]\n\n\n\n\nCode\ncorrelation_features_transformed = X_train_transformed.corr()\ncorrelation_features_transformed[np.abs(correlation_features_transformed) &gt; 0.3].sum() \n\n# sns.heatmap(correlation_features_transformed[np.abs(correlation_features_transformed) &gt; 0.3])\n\nsns.heatmap(correlation_features[np.abs(correlation_features) &gt; 0.3])"
  },
  {
    "objectID": "posts/icr-kaggle/index.html#train-first-ml-models",
    "href": "posts/icr-kaggle/index.html#train-first-ml-models",
    "title": "Identify age-related conditions Competition",
    "section": "Train first ML Models",
    "text": "Train first ML Models\nUntil now, we have conducted data exploration, preprocessed adequately by scaling and imputing missing values, and split them in a way that the class distribution is preserved between sets.\nAfter all these steps, we can build our first machine learning models.\nThe performance of the model will be measured using balanced_log_loss, which is defined in the next cell. This type of loss function takes into account the imbalance of the data and penalizes the wrong predictions for class 1.\nWe will try to fit certain algorithms and eventually we will use an ensemble model.\n\n\nCode\nfrom sklearn.metrics import accuracy_score, roc_auc_score, recall_score, f1_score, confusion_matrix, log_loss, make_scorer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.svm import SVC\nimport xgboost as xgb\n\n\n# Use balanced log loss as metric\ndef balanced_log_loss(y_true, y_pred):\n    nc = np.bincount(y_true)\n    return log_loss(y_true, y_pred, sample_weight = 1/nc[y_true], eps=1e-15)\n\n\n\n\nCode\nparams = {\n    # 'base_score'       : [0.50, 0.60, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95],\n    # \"learning_rate\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30],\n    \"n_estimators\"     : [100, 200, 300, 400, 500, 600, 800, 1000],\n    \"max_depth\"        : [3, 4, 5, 6, 8, 10, 12, 15],\n    # 'min_samples_split': [0.10, 0.12, 0.15, 0.17, 0.20, 0.22, 0.25]\n    # \"min_child_weight\" : [1, 3, 5, 7],\n    # \"gamma\"            : [ 0.0, 0.1, 0.2, 0.3, 0.4],\n    # \"colsample_bytree\" : [ 0.3, 0.4, 0.5, 0.7],\n}\n\nclassifier = xgb.XGBClassifier(random_state = seed, scale_pos_weight = positive_weight)\nrandom_search = RandomizedSearchCV(classifier, param_distributions=params, n_iter = 5, scoring = make_scorer(balanced_log_loss, greater_is_better=True),\n                                  n_jobs = -1, cv = 5, verbose = 3)\nrandom_search.fit(X_train, y_train)\n\n\n\n\nCode\nrandom_search.best_estimator_\nrandom_search.best_params_\n\n\n\n\n\nCode\nfrom sklearn.model_selection import KFold, StratifiedKFold, cross_validate, cross_val_score\n\nxgb_clf = random_search.best_estimator_\n\nxgb_score = cross_val_score(xgb_clf, X_train, y_train, cv = 5, groups = y_train, scoring = make_scorer(balanced_log_loss, greater_is_better=True))\nprint(f\"Score from cross_val_score is: {list(map('{:.2f}'.format,xgb_score))}\")\n\nnp.mean(xgb_score)\n\n\n\nCheck that the results of the out of fold are consistent with the cross-validation scores\n\n\nCode\ny_pred_oof = xgb_clf.predict(X_validation)\nscore_oof = balanced_log_loss(y_validation, y_pred_oof)\nscore_oof\n\n\n\nTry other classifiers\nSome people during the Kaggle competition reported the successful use of lightgbm and catboost. We will use them in the next cells\n\n\nCode\nimport lightgbm as lgb\nimport optuna\n# import shap\n\nlgbm_clf = lgb.LGBMClassifier(n_estimators=150,\n                        learning_rate=0.1,\n                        num_leaves=10,\n                        max_depth=4,\n                        subsample = 0.9,\n                        colsample_bytree = 0.3,\n                        subsample_freq = 3,\n                        min_split_gain = 0.1,\n                        min_child_weight = 0.0005,\n                        min_child_samples = 20,\n                        reg_lambda = 0.01,\n                        reg_alpha = 0.1,\n                        objective = 'binary',\n                        class_weight = 'balanced',\n                        random_state=seed)\n\nlgbm_clf.fit(X_train, y_train)\n\nscore_lgbm = cross_val_score(lgbm_clf, X_train, y_train, cv=5, n_jobs=-1, \n                             scoring = make_scorer(balanced_log_loss, greater_is_better=True))\nscore_lgbm\n\n\n\n\n\nCode\n\nsvc_clf = SVC(random_state=seed, degree= 3, probability= True)\nsvc_clf.fit(X_train, y_train)\n\nscore_svc = cross_val_score(svc_clf, X_train, y_train, cv=5, n_jobs=-1, \n                            scoring = make_scorer(balanced_log_loss, greater_is_better=True))\nscore_svc\n\n\n\n\n\nCode\nfrom catboost import CatBoostClassifier\n\ncatboost_clf = CatBoostClassifier(\n    iterations = 250,\n    learning_rate = 0.04525759098022261,\n    depth = 7, # depth above 10 usually introduces overfitting\n    l2_leaf_reg = 7.82,\n    early_stopping_rounds = 3,\n    auto_class_weights = 'Balanced',\n    loss_function = 'Logloss',\n    verbose = 0,\n)\n\ncatboost_clf.fit(X_train, y_train)\nscore_catboost = cross_val_score(catboost_clf, X_train, y_train, cv=5, n_jobs=-1, \n                                 scoring = make_scorer(balanced_log_loss, greater_is_better=True))\nscore_catboost\n\n# best_hyperparams_cat  = {'iterations': 245, 'learning_rate': 0.04525759098022261, 'depth': 7, 'l2_leaf_reg': 7.822788431713332, 'early_stopping_rounds': 3}\n\n\n\n\nCode\nrf_clf = RandomForestClassifier(n_estimators = 100, random_state = seed, \n                                criterion= 'log_loss', \n                                max_depth=7, class_weight= 'balanced')\n\nrf_clf.fit(X_train, y_train)\nscore_rf = cross_val_score(rf_clf, X_train, y_train, cv=5, n_jobs=-1, \n                           scoring = make_scorer(balanced_log_loss, greater_is_better=True))\nscore_rf\n\n\n\n\nStacking classifier\n\n\nCode\nfrom sklearn.ensemble import StackingClassifier\n\nestimators = [\n    ('LogisticRegression', LogisticRegression(max_iter=10000)), #doesn't converge\n    ('RandomForestClassifier', rf_clf),\n    ('CatBoostClassifier', catboost_clf),\n    ('XGBClassifier',  xgb_clf),\n    # ('LGBMClassifier', lgbm_clf)\n]\n\nstacked_clf = StackingClassifier(estimators=estimators, \n                                   final_estimator=lgbm_clf\n                                   )\n\nstacked_clf.fit(X_train, y_train)\nstacked_score = cross_val_score(stacked_clf, X_train, y_train, cv=5, n_jobs=-1, \n                                scoring = make_scorer(balanced_log_loss, greater_is_better=True))\n\n\n\n\nCode\nstacked_score.mean()\n\n\n\n\nCode\nprint(f\"Stacked score is {stacked_score}\")\n\ny_pred_oof_stacked = stacked_clf.predict(X_validation)\nscore_oof_stacked = balanced_log_loss(y_validation, y_pred_oof_stacked)\nscore_oof_stacked\n\n\nLetś plot the importance of the tree model and the tree itself\n\n\nInference with Test Data\nOnce we have classified our train model, we will classify our test data.\nHere is when we have to use again the same pipeline used in the train set to transform the data on the test set.\n\n\nCode\ntest_data = pd.read_csv(\"/kaggle/input/icr-identify-age-related-conditions/test.csv\", index_col=\"Id\")\ntest_data\n\nX_test = test_data[features]\n\nX_test_transformed = data_pipeline.transform(X_test)\n\n\n\n\nCode\n\n# Fit Model to test data\nclf = stacked_clf\ny_pred = clf.predict_proba(X_test_transformed)\ny_pred\n\nsubmission_df = pd.DataFrame(y_pred, columns=['class_0', 'class_1'], \n                             index=X_test_transformed.index)\nsubmission_df.reset_index(inplace=True)\nsubmission_df\n\n\n\n\n\nCode\nsubmission_df.to_csv(\"/kaggle/working/submission.csv\", index=False)"
  },
  {
    "objectID": "posts/icr-kaggle/index_old.html",
    "href": "posts/icr-kaggle/index_old.html",
    "title": "ICR Age-Related Conditions Kaggle Competition",
    "section": "",
    "text": "This is a post to explain how a simulation conducted to evaluate the impact of infected visitors to international large events was conducted.\nThis work was highlighted in local media showcasing the impact of modelling in the decision-making for Expo2020 in [this article] and [this article]. In addition, a MoU between Department of Health of Abu Dhabi and Khalifa University was signed at Expo.\n\n\n\nFigure 1: Template of the online model simulator for COVID-19\n\n\nThe model simulator can be seen in Figure 1."
  },
  {
    "objectID": "posts/icr-kaggle/icr-identifying-age-related-conditions-xgboost.html",
    "href": "posts/icr-kaggle/icr-identifying-age-related-conditions-xgboost.html",
    "title": "",
    "section": "",
    "text": "# Table of Contents 1. Methodology 1. Importing Libraries\n1. Reading Data 1. Check for Null Values 1. Split Dataframe\n1. X 1. y 1. Encoding Categorical Values\n1. Split Data into Train/Test 1. EC1 1. EC2 1. Model Creation\n1. Setting Up Model\n1. Fitting Model\n1. KFold Accuracy\n1. Feature Importance\n1. Applying Model to test.csv 1. Predicting Outcome\n\nMethodology\nTOC\nThis was an interesting and challenging dataset to work with. In this attempt, I decided not to use greeks.csv file at all and I am not quite sure how it would affect the overall performance. My approach was to get to the predictions with the least complicated or more straight forward dataset so I only used train.csv. In this notebook, I utilized XGBOOST as my go-to ML library.\n\nVersion 1: There was no use of scale_pos_weight.\nMany Versions Later\nVersion 10: Performed multiple GridSearch with different scale_pos_weight values.\nVersion 11: Lost in the Matrix and started trying different scale_pos_weight and decided to fill NaN values with the mean of those columns.\n\n\n\n\n\n\n1. Importing Libraries\nTOC\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport re\nimport category_encoders as ce\nfrom collections import Counter\nfrom xgboost import XGBClassifier\nfrom xgboost import plot_tree\nfrom xgboost import plot_importance\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import StratifiedKFold\nimport seaborn as sns\nimport matplotlib\nfrom matplotlib import pyplot\n%matplotlib inline\n\n\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version &gt;=1.16.5 and &lt;1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version &gt;={np_minversion} and &lt;{np_maxversion}\"\n\n\n\n\n\n\n\n2. Reading Data\nTOC\n::: {.cell _cell_guid=‘b1076dfc-b9ad-4769-8c92-a6c4dae69d19’ _uuid=‘8f2839f25d086af736a60e9eeb907d3b93b6e0e5’ execution=‘{“iopub.execute_input”:“2023-07-20T00:31:41.555926Z”,“iopub.status.busy”:“2023-07-20T00:31:41.554942Z”,“iopub.status.idle”:“2023-07-20T00:31:41.561750Z”,“shell.execute_reply”:“2023-07-20T00:31:41.560620Z”}’ papermill=‘{“duration”:0.021389,“end_time”:“2023-07-20T00:31:41.563781”,“exception”:false,“start_time”:“2023-07-20T00:31:41.542392”,“status”:“completed”}’ tags=‘[]’ execution_count=2}\n\nCode\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\n/kaggle/input/icr-identify-age-related-conditions/sample_submission.csv\n/kaggle/input/icr-identify-age-related-conditions/greeks.csv\n/kaggle/input/icr-identify-age-related-conditions/train.csv\n/kaggle/input/icr-identify-age-related-conditions/test.csv\n\n:::\n\n\nCode\ndf = pd.read_csv(\"/kaggle/input/icr-identify-age-related-conditions/train.csv\")\ndf.head(2)\n\n\n\n\n\n\n\n\n\nId\nAB\nAF\nAH\nAM\nAR\nAX\nAY\nAZ\nBC\n...\nFL\nFR\nFS\nGB\nGE\nGF\nGH\nGI\nGL\nClass\n\n\n\n\n0\n000ff2bfdfe9\n0.209377\n3109.03329\n85.200147\n22.394407\n8.138688\n0.699861\n0.025578\n9.812214\n5.555634\n...\n7.298162\n1.73855\n0.094822\n11.339138\n72.611063\n2003.810319\n22.136229\n69.834944\n0.120343\n1\n\n\n1\n007255e47698\n0.145282\n978.76416\n85.200147\n36.968889\n8.138688\n3.632190\n0.025578\n13.517790\n1.229900\n...\n0.173229\n0.49706\n0.568932\n9.292698\n72.611063\n27981.562750\n29.135430\n32.131996\n21.978000\n0\n\n\n\n\n2 rows × 58 columns\n\n\n\nThe Id column isn’t necessary and we can drop it.\n\n\nCode\ndf.drop(['Id'], axis=1, inplace=True)\n\n\nLet’s find if there are any columns with categorical values.\n\n\nCode\ncategorical_columns = df.select_dtypes(include=['object']).columns.tolist()\n\n\nWe can now perform a quick analysis on this column.\n\n\nCode\ndef sum_categorical(column_names):\n    for values in column_names:\n        print(\"Summary for {0}\".format(values))\n        print(df[values].value_counts())\n        print(\"--------------------------\")\n\n\n\n\nCode\nsum_categorical(categorical_columns)\n\n\nSummary for EJ\nB    395\nA    222\nName: EJ, dtype: int64\n--------------------------\n\n\nSo the column EJ has categorical values with two distinct categories A and B. Let’s see if we have an imbalanced dataset.\n\n\nCode\nsns.countplot ( x = \"Class\", hue=\"EJ\", data = df )\npyplot.show()\n\n\n\n\n\nYes, the quick visualization shows that we might have an imbalanced dataset.\n\n\n\n\n\n2 - A. Check for Null Values\nTOC\n\n\nCode\ndf.isnull().values.any()\n\n\nTrue\n\n\n\nThere are null values and we need to find what columns have null values ⚠️\n\nThe following function went over each column and summarized if there were any null values as another way of checking.\n\n\nCode\ncolumn_names = list(df.columns.values)\ndef check_nan(column_names):\n    for column in column_names:\n        if df[column].isnull().sum() != 0:\n            print(\"column: {0} --&gt; {1}\".format(column, df[column].isnull().sum()))\n        \ncheck_nan(column_names)\n\n\ncolumn: BQ --&gt; 60\ncolumn: CB --&gt; 2\ncolumn: CC --&gt; 3\ncolumn: DU --&gt; 1\ncolumn: EL --&gt; 60\ncolumn: FC --&gt; 1\ncolumn: FL --&gt; 1\ncolumn: FS --&gt; 2\ncolumn: GL --&gt; 1\n\n\nWe can fill null values as follows and see how the model would perform.\n\n\nCode\ndef fill_nan(column_names):\n    for column in column_names:\n        if df[column].isnull().sum() != 0:\n            mean = df[column].mean()\n            df[column].fillna(mean, inplace = True)\n\n\n\n\nCode\nfill_nan(column_names)\n\n\n\n\nCode\ndf.isnull().values.any()\n\n\nFalse\n\n\n\n\n\n\n\n3. Split Dataframe\nTOC\n\n\n\n\n\n3 - A. X\nTOC\n\n\nCode\nX = df.iloc[:, :-1]\nX.head(2)\n\n\n\n\n\n\n\n\n\nAB\nAF\nAH\nAM\nAR\nAX\nAY\nAZ\nBC\nBD\n...\nFI\nFL\nFR\nFS\nGB\nGE\nGF\nGH\nGI\nGL\n\n\n\n\n0\n0.209377\n3109.03329\n85.200147\n22.394407\n8.138688\n0.699861\n0.025578\n9.812214\n5.555634\n4126.58731\n...\n3.583450\n7.298162\n1.73855\n0.094822\n11.339138\n72.611063\n2003.810319\n22.136229\n69.834944\n0.120343\n\n\n1\n0.145282\n978.76416\n85.200147\n36.968889\n8.138688\n3.632190\n0.025578\n13.517790\n1.229900\n5496.92824\n...\n10.358927\n0.173229\n0.49706\n0.568932\n9.292698\n72.611063\n27981.562750\n29.135430\n32.131996\n21.978000\n\n\n\n\n2 rows × 56 columns\n\n\n\n\n\n\n\n\n3. y\nTOC\n\n\nCode\ny = df.iloc[:,-1]\ny\n\n\n0      1\n1      0\n2      0\n3      0\n4      1\n      ..\n612    0\n613    0\n614    0\n615    0\n616    0\nName: Class, Length: 617, dtype: int64\n\n\n\n\nCode\ncounter = Counter(y)\nprint(counter)\n\n\nCounter({0: 509, 1: 108})\n\n\n\n\n\n\n\n4. Encoding Categorical Values\nTOC\nFor this, I decided to use OneHotEncoder from category_encoders library.\n\n\nCode\nenc = ce.OneHotEncoder(cols=[\"EJ\"])\nenc.fit(X)\nencoded_x = enc.transform(X)\nencoded_x.head(2)\n\n\n\n\n\n\n\n\n\nAB\nAF\nAH\nAM\nAR\nAX\nAY\nAZ\nBC\nBD\n...\nFI\nFL\nFR\nFS\nGB\nGE\nGF\nGH\nGI\nGL\n\n\n\n\n0\n0.209377\n3109.03329\n85.200147\n22.394407\n8.138688\n0.699861\n0.025578\n9.812214\n5.555634\n4126.58731\n...\n3.583450\n7.298162\n1.73855\n0.094822\n11.339138\n72.611063\n2003.810319\n22.136229\n69.834944\n0.120343\n\n\n1\n0.145282\n978.76416\n85.200147\n36.968889\n8.138688\n3.632190\n0.025578\n13.517790\n1.229900\n5496.92824\n...\n10.358927\n0.173229\n0.49706\n0.568932\n9.292698\n72.611063\n27981.562750\n29.135430\n32.131996\n21.978000\n\n\n\n\n2 rows × 57 columns\n\n\n\n\n\n\n\n\n5. Split Data into Train/Test\nTOC\n\n\nCode\nseed = 0\ntest_size = 0.2\nX_train, X_test, y_train, y_test = train_test_split(encoded_x, y, test_size = test_size,\n                                                    random_state = seed)\nec1_eval_set = [(X_train, y_train), (X_test, y_test)]\n\n\n\n\nCode\nprint(\"train y shape: {0} and test y shape: {1}\".format(y_train.shape, y_test.shape))\n\n\ntrain y shape: (493,) and test y shape: (124,)\n\n\n\n\nCode\npositive_records = y_train.sum()\nnegative_records = len(y_train) - positive_records\nestimate = negative_records / positive_records\nprint('y Estimate: %.3f' % estimate)\n\n\ny Estimate: 4.478\n\n\nThe Counter method confirms that we have an imbalanced dataset. We can utilize estimating the value for the scale_pos_weight XGBoost hyperparameter (I ended up performing a lengthy grid search to find the right value).\n\n\n\n\n\n6. Model Creation\nTOC\n\n\n\n\n\n6 - A. Setting Up Model\nTOC\n\n\nCode\nmodel = XGBClassifier(colsample_bylevel = 0.6, \n                      colsample_bytree = 1.0, \n                      learning_rate = 0.3, \n                      max_depth = 2, \n                      n_estimators = 100, \n                      subsample = 1.0,   \n                      scale_pos_weight = 5)\n\n\n\n\n\n\n\n6 - B. Fitting Model\nTOC\n\n\nCode\nmodel.fit(X_train, y_train, early_stopping_rounds = 10, eval_metric = [\"error\", \"logloss\"],\n          eval_set = ec1_eval_set, verbose = True)\n\n\n[0] validation_0-error:0.29412  validation_0-logloss:0.58808    validation_1-error:0.44355  validation_1-logloss:0.64736\n[1] validation_0-error:0.12373  validation_0-logloss:0.49848    validation_1-error:0.24194  validation_1-logloss:0.58199\n[2] validation_0-error:0.13185  validation_0-logloss:0.44767    validation_1-error:0.24194  validation_1-logloss:0.53391\n[3] validation_0-error:0.12576  validation_0-logloss:0.39804    validation_1-error:0.23387  validation_1-logloss:0.49501\n[4] validation_0-error:0.09736  validation_0-logloss:0.35618    validation_1-error:0.19355  validation_1-logloss:0.45094\n[5] validation_0-error:0.07708  validation_0-logloss:0.32159    validation_1-error:0.14516  validation_1-logloss:0.42632\n[6] validation_0-error:0.07099  validation_0-logloss:0.30280    validation_1-error:0.13710  validation_1-logloss:0.40379\n[7] validation_0-error:0.05882  validation_0-logloss:0.28120    validation_1-error:0.11290  validation_1-logloss:0.38709\n[8] validation_0-error:0.05477  validation_0-logloss:0.26229    validation_1-error:0.12097  validation_1-logloss:0.38202\n[9] validation_0-error:0.05071  validation_0-logloss:0.24660    validation_1-error:0.11290  validation_1-logloss:0.36577\n[10]    validation_0-error:0.04868  validation_0-logloss:0.23019    validation_1-error:0.10484  validation_1-logloss:0.35071\n[11]    validation_0-error:0.05274  validation_0-logloss:0.21660    validation_1-error:0.08871  validation_1-logloss:0.34289\n[12]    validation_0-error:0.04665  validation_0-logloss:0.20364    validation_1-error:0.08065  validation_1-logloss:0.34237\n[13]    validation_0-error:0.04665  validation_0-logloss:0.19335    validation_1-error:0.11290  validation_1-logloss:0.34161\n[14]    validation_0-error:0.04057  validation_0-logloss:0.18261    validation_1-error:0.09677  validation_1-logloss:0.33422\n[15]    validation_0-error:0.03448  validation_0-logloss:0.17262    validation_1-error:0.08065  validation_1-logloss:0.33556\n[16]    validation_0-error:0.03043  validation_0-logloss:0.16510    validation_1-error:0.08065  validation_1-logloss:0.33705\n[17]    validation_0-error:0.03043  validation_0-logloss:0.15841    validation_1-error:0.08871  validation_1-logloss:0.33347\n[18]    validation_0-error:0.03245  validation_0-logloss:0.15172    validation_1-error:0.09677  validation_1-logloss:0.33277\n[19]    validation_0-error:0.02434  validation_0-logloss:0.14562    validation_1-error:0.09677  validation_1-logloss:0.33283\n[20]    validation_0-error:0.02231  validation_0-logloss:0.14060    validation_1-error:0.08871  validation_1-logloss:0.32300\n[21]    validation_0-error:0.02231  validation_0-logloss:0.13452    validation_1-error:0.08065  validation_1-logloss:0.31742\n[22]    validation_0-error:0.01826  validation_0-logloss:0.12782    validation_1-error:0.08871  validation_1-logloss:0.31013\n[23]    validation_0-error:0.01826  validation_0-logloss:0.11980    validation_1-error:0.08871  validation_1-logloss:0.30553\n[24]    validation_0-error:0.01420  validation_0-logloss:0.11342    validation_1-error:0.08065  validation_1-logloss:0.30490\n[25]    validation_0-error:0.00811  validation_0-logloss:0.10704    validation_1-error:0.08065  validation_1-logloss:0.28931\n[26]    validation_0-error:0.01014  validation_0-logloss:0.10330    validation_1-error:0.08065  validation_1-logloss:0.29237\n[27]    validation_0-error:0.01217  validation_0-logloss:0.09848    validation_1-error:0.08065  validation_1-logloss:0.29143\n[28]    validation_0-error:0.00609  validation_0-logloss:0.09390    validation_1-error:0.08065  validation_1-logloss:0.29755\n[29]    validation_0-error:0.00811  validation_0-logloss:0.08982    validation_1-error:0.08065  validation_1-logloss:0.29234\n[30]    validation_0-error:0.00609  validation_0-logloss:0.08689    validation_1-error:0.08065  validation_1-logloss:0.28937\n[31]    validation_0-error:0.00609  validation_0-logloss:0.08223    validation_1-error:0.08065  validation_1-logloss:0.28406\n[32]    validation_0-error:0.00609  validation_0-logloss:0.07949    validation_1-error:0.08065  validation_1-logloss:0.28372\n[33]    validation_0-error:0.00609  validation_0-logloss:0.07594    validation_1-error:0.08065  validation_1-logloss:0.27781\n[34]    validation_0-error:0.00406  validation_0-logloss:0.07320    validation_1-error:0.08065  validation_1-logloss:0.27534\n[35]    validation_0-error:0.00406  validation_0-logloss:0.07060    validation_1-error:0.08065  validation_1-logloss:0.27715\n[36]    validation_0-error:0.00406  validation_0-logloss:0.06800    validation_1-error:0.08065  validation_1-logloss:0.27471\n[37]    validation_0-error:0.00203  validation_0-logloss:0.06570    validation_1-error:0.08065  validation_1-logloss:0.27701\n[38]    validation_0-error:0.00203  validation_0-logloss:0.06301    validation_1-error:0.08065  validation_1-logloss:0.26919\n[39]    validation_0-error:0.00203  validation_0-logloss:0.06110    validation_1-error:0.08065  validation_1-logloss:0.26953\n[40]    validation_0-error:0.00203  validation_0-logloss:0.05902    validation_1-error:0.08065  validation_1-logloss:0.27136\n[41]    validation_0-error:0.00203  validation_0-logloss:0.05723    validation_1-error:0.08065  validation_1-logloss:0.27007\n[42]    validation_0-error:0.00203  validation_0-logloss:0.05447    validation_1-error:0.08065  validation_1-logloss:0.26840\n[43]    validation_0-error:0.00203  validation_0-logloss:0.05171    validation_1-error:0.08065  validation_1-logloss:0.26158\n[44]    validation_0-error:0.00203  validation_0-logloss:0.04959    validation_1-error:0.08065  validation_1-logloss:0.25788\n[45]    validation_0-error:0.00000  validation_0-logloss:0.04811    validation_1-error:0.07258  validation_1-logloss:0.25949\n[46]    validation_0-error:0.00000  validation_0-logloss:0.04666    validation_1-error:0.08065  validation_1-logloss:0.25713\n[47]    validation_0-error:0.00000  validation_0-logloss:0.04555    validation_1-error:0.08065  validation_1-logloss:0.26138\n[48]    validation_0-error:0.00000  validation_0-logloss:0.04354    validation_1-error:0.08065  validation_1-logloss:0.26857\n[49]    validation_0-error:0.00000  validation_0-logloss:0.04181    validation_1-error:0.08065  validation_1-logloss:0.26759\n[50]    validation_0-error:0.00000  validation_0-logloss:0.04000    validation_1-error:0.08065  validation_1-logloss:0.26794\n[51]    validation_0-error:0.00000  validation_0-logloss:0.03863    validation_1-error:0.08065  validation_1-logloss:0.26268\n[52]    validation_0-error:0.00000  validation_0-logloss:0.03715    validation_1-error:0.08065  validation_1-logloss:0.26126\n[53]    validation_0-error:0.00000  validation_0-logloss:0.03616    validation_1-error:0.08065  validation_1-logloss:0.26395\n[54]    validation_0-error:0.00000  validation_0-logloss:0.03471    validation_1-error:0.08065  validation_1-logloss:0.26047\n[55]    validation_0-error:0.00000  validation_0-logloss:0.03313    validation_1-error:0.08065  validation_1-logloss:0.25964\n\n\n/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:835: UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:835: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n\n\nXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=0.6, colsample_bynode=None,\n              colsample_bytree=1.0, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=0.3, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=2, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n              predictor=None, random_state=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBClassifierXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=0.6, colsample_bynode=None,\n              colsample_bytree=1.0, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=0.3, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=2, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n              predictor=None, random_state=None, ...)\n\n\n\n\n\n\n\n7. KFold Accuracy\nTOC\n\n\nCode\nKfold = KFold(n_splits = 10)\nresults_K = cross_val_score(model, encoded_x, y, cv = Kfold)\nprint(\"KFold - Accuracy: {0}% ({1})%)\".format(results_K.mean()*100, results_K.std()*100))\n\n\nKFold - Accuracy: 93.68588048651509% (2.6552857153256966)%)\n\n\nWith the new parameters, we got about 94% accuracy. Let’s see how we do.\n\n\n\n\n\n8. Feature Importance\nTOC\n\n\nCode\nplot_importance(model)\npyplot.show()\n\n\n\n\n\n\nFeature Importance: Maybe anything lower than 5.0 can be dropped. 🤔\n\n\n\n\n\n\n9. Correlation\n\nWe can encode the whole dataframe without splitting into X and y\n\n\n\nCode\ndf_enc = ce.OneHotEncoder(cols=[\"EJ\"])\nencoded_df = df_enc.fit_transform(df)\ncorr = encoded_df.corr()\n\n\n\n\nCode\npyplot.figure(figsize=(40,30))\nsns.heatmap(corr,annot=True,cmap='BuPu')\n\n\n&lt;Axes: &gt;\n\n\n\n\n\n\nCorrelation: At first glance, the feature importance and correlation graphs seem to be agreeing to certain point. 🤔\n\n\n\n\n\n\n10. Applying Model to test.csv\nTOC\n\n\nCode\ntest_df = pd.read_csv(\"/kaggle/input/icr-identify-age-related-conditions/test.csv\")\ntest_df.head(2)\n\n\n\n\n\n\n\n\n\nId\nAB\nAF\nAH\nAM\nAR\nAX\nAY\nAZ\nBC\n...\nFI\nFL\nFR\nFS\nGB\nGE\nGF\nGH\nGI\nGL\n\n\n\n\n0\n00eed32682bb\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\n010ebe33f668\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n2 rows × 57 columns\n\n\n\nLet’s check for null values.\n\n\nCode\ntest_df.isnull().values.any()\n\n\nFalse\n\n\nGreat, no null vaues. We can also drop the Id column and save it to a different dataframe so that we can append our predictions back to the original test.csv dataframe.\n\n\nCode\nnew_test_df = test_df.drop(['Id'], axis = 1)\nnew_test_df.head(2)\n\n\n\n\n\n\n\n\n\nAB\nAF\nAH\nAM\nAR\nAX\nAY\nAZ\nBC\nBD\n...\nFI\nFL\nFR\nFS\nGB\nGE\nGF\nGH\nGI\nGL\n\n\n\n\n0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n2 rows × 56 columns\n\n\n\nLet’s encode the EJ column as well.\n\n\nCode\nencoded_test_df = enc.transform(new_test_df)\nencoded_test_df.head(2)\n\n\n\n\n\n\n\n\n\nAB\nAF\nAH\nAM\nAR\nAX\nAY\nAZ\nBC\nBD\n...\nFI\nFL\nFR\nFS\nGB\nGE\nGF\nGH\nGI\nGL\n\n\n\n\n0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n2 rows × 57 columns\n\n\n\n\n\n\n\n\n11. Predicting Outcome\nTOC\nLet’s predict the class_0 first.\n\n\nCode\nclass_0_pred = model.predict_proba(encoded_test_df)[:,0]\n\n\nAnd now, class_1.\n\n\nCode\nclass_1_pred = model.predict_proba(encoded_test_df)[:,1]\n\n\nLet’s append the predictions back to our test.csv dataframe.\n\n\nCode\ntest_df[\"class_0\"] = pd.Series(class_0_pred).values\ntest_df[\"class_1\"] = pd.Series(class_1_pred).values\n\n\nAnother dataframe with the columns we need.\n\n\nCode\nsubmission_df = test_df[[\"Id\", \"class_0\", \"class_1\"]]\nsubmission_df\n\n\n\n\n\n\n\n\n\nId\nclass_0\nclass_1\n\n\n\n\n0\n00eed32682bb\n0.321698\n0.678302\n\n\n1\n010ebe33f668\n0.321698\n0.678302\n\n\n2\n02fa521e1838\n0.321698\n0.678302\n\n\n3\n040e15f562a2\n0.321698\n0.678302\n\n\n4\n046e85c7cc7f\n0.321698\n0.678302\n\n\n\n\n\n\n\n\n\nCode\nsubmission_df.to_csv(\"/kaggle/working/submission.csv\", index=False)"
  },
  {
    "objectID": "posts/data-analysis-post/index.html",
    "href": "posts/data-analysis-post/index.html",
    "title": "Data analysis to evaluate the hospitalisation risk between SARS-CoV-2 Variants",
    "section": "",
    "text": "In this report, an analysis of the data to be used to estimate epidemiological parameters for the different SARS-CoV-2 lineages of interest (e.g.: BA.1, B.1.629, etc.) is presented. Firstly, the data is cleaned to ensure that no duplicates, and proper lineages are used. Then, the data analysis is conducted. In particular, the analysis is focused on the identification of potential biases in the data as it compares people who were sequenced against people who were not sequenced.\n\n\n\n\n\n\nWarning\n\n\n\nDue to confidentiality issues, the results have been masked in this document accordingly.\nThe data displayed in the figures, tables and text are shown for illustrative purposes and do not reflect real numbers.\n\n\n\n\nThe sampling strategy followed in the United Arab Emirates from January 1st, 2022 comprised the sequencing of samples from individuals that belonged to one of the following groups:\n\nAll people in ICU/HDU and/or deceased.\nAll people who entered through a port of entry and had a positive PCR.\nThe rest of the infected people were sequenced based on a random sample proportionally to the number of positives reported by each lab on a weekly basis.\n\nSampling prior to the stated day needs to be double-checked for proper interpretation.\nThe sampling strategy described follows WHO recommendation. Therefore, it might be of interest to develop a methodology in which an unbiased (or systematically low bias) estimator of the sample is found after following WHO recommendations for sampling COVID-19 patients.\nFirst, we are going to load the dataset and examine it:\n\n\n'data.frame':   2535161 obs. of  34 variables:\n $ PseudoID                   : chr  \"{FA0C53C0-82F7-4803-AF51-F555BA559850}\" \"{FF55A071-A266-4A57-BE88-F555BA825A10}\" \"{973E11D1-51F1-4069-96D7-F55625907981}\" \"{B37D192F-AA2C-4812-890B-F5573B6873D2}\" ...\n $ Lineage                    : chr  \"B.1.617.2\" \"\" \"B.1.617.2\" \"\" ...\n $ CollectionDateTime         : chr  \"2021-11-24 00:00:00\" \"2022-08-08 00:00:00\" \"2021-06-09 00:00:00\" \"2022-06-08 00:00:00\" ...\n $ Dose1_Vaccine_Type         : chr  \"Pfizer\" \"Pfizer\" \"\" \"Sinopharm\" ...\n $ Dose1_VaccinationDate      : chr  \"2022-03-13\" \"2021-09-15\" \"\" \"2021-05-02\" ...\n $ Dose2_Vaccine_Type         : chr  \"\" \"Pfizer\" \"\" \"Sinopharm\" ...\n $ Dose2_VaccinationDate      : chr  \"\" \"2021-10-06\" \"\" \"2021-05-23\" ...\n $ Dose3_Vaccine_Type         : chr  \"\" \"\" \"\" \"Sinopharm\" ...\n $ Dose3_VaccinationDate      : chr  \"\" \"\" \"\" \"2021-12-21\" ...\n $ Dose4_Vaccine_Type         : chr  \"\" \"\" \"\" \"\" ...\n $ Dose4_VaccinationDate      : chr  \"\" \"\" \"\" \"\" ...\n $ VaccinationStatus          : chr  \"NotVaccincated\" \"Full\" \"NotVaccincated\" \"Full\" ...\n $ NoOfDoses                  : int  1 2 0 3 0 3 3 0 3 3 ...\n $ LineageHospitalized        : int  0 1 0 0 0 0 0 0 0 0 ...\n $ IsDied                     : int  0 0 0 0 0 0 0 0 0 0 ...\n $ Death_Date                 : chr  \"\" \"\" \"\" \"\" ...\n $ LOS                        : int  NA 12 2 NA NA NA NA NA NA NA ...\n $ hospital_discharge_date    : chr  \"\" \"2022-08-22 11:59:00\" \"2022-12-21 13:28:00\" \"\" ...\n $ Date_hospitalization       : chr  \"\" \"2022-08-10 18:11:00.000\" \"2022-12-19 09:23:43.000\" \"\" ...\n $ Covid_Variant              : chr  \"Delta\" \"\" \"Delta\" \"\" ...\n $ Gender                     : chr  \"Male\" \"Male\" \"Female\" \"Male\" ...\n $ Nationality                : chr  \"Expatraite\" \"PHILIPPINES\" \"UNITED ARAB EMIRATES\" \"UNITED ARAB EMIRATES\" ...\n $ LineageRecorded            : chr  \"\" \"\" \"\" \"\" ...\n $ Comorbities                : int  0 0 0 0 0 0 0 0 0 0 ...\n $ PriorInfection             : int  NA NA 1 NA NA NA NA NA 1 1 ...\n $ PriorInfection_Count       : int  NA NA 4 NA NA NA NA NA 2 2 ...\n $ Band15Years                : chr  \"15-29\" \"30-44\" \"60+\" \"45-59\" ...\n $ Tableau_ICU_Status         : chr  \"Others\" \"\" \"Others\" \"\" ...\n $ Tableau_Travel_Country_Name: chr  \"\" \"No\" \"UNKNOWN\" \"No\" ...\n $ Performing.Facility        : chr  \"TW Tawam Hospital\" \"Mediclinic\" \"\" \"\" ...\n $ Sequenced                  : chr  \"1\" \"0\" \"1\" \"0\" ...\n $ AGE                        : chr  \"21\" \"33\" \"87\" \"49\" ...\n $ CASES                      : chr  \"146\" \"1475\" \"4254\" \"4052\" ...\n $ TESTED                     : int  182946 173947 144956 165451 178620 277402 260176 123265 298367 248765 ...\n\n\nAs we see, we have information per patient for the following variables:\n\nPseudoID: Unique identifier for each patient.\nLineage: Identification of the lineage sequenced.\nCollectionDateTime: Date and time in which the PCR sample was collected for the infected patient.\nDose_VaccinationDate: Date in which the vaccination of the dose referred was taken.\nDose_Vaccine_Type: Type of vaccine that was taken.\nVaccination Status: Fully vaccinated if the individual got at least two doses or unvaccinated.\nNoOfDoses: Number of vaccine doses received per individual.\nLineageHospitalized: Identification of hospitalization for the patient who was sequenced.\nIsDied: Identification of death for the patient.\nDeath_Date: Date in which the patient died.\nLOS: Length of stay in the hospital.\nHospital_Discharge_date: Date in which the patient was discharged.\nDate_hospitalization: Date in which the patient was admitted.\nGender: Male or Female.\nNationality: Expatriate or Local.\nCovid_Variant: Recoding of the variant of the COVID Lineage.\nComorbidities: Number of comorbidities per patient.\nPriorInfection: Flag if the patient registered a prior infection before the current positive.\nPriorInfection_Count: Number of previous infections registered.\nAge: Age of the patient.\nBand15Years: Age group of 15 years old for the patient.\nSequenced: Identification for patients that were sequenced or not sequenced.\nPerforming.Facility: Lab conducting the collection of the sample that was sequencing. This field will be used to estimate the population properties of those that were randomly sequenced. In particular, checking the randomness assumption is a goal of the work presented in this document. (Group 3)\nTableau_ICU_Status: Label if the patient is ICU/HDU or dead. This field will be used to label the patients as HDU/ICU pr dead as reason per sequencing. All patients labeled in this field were sequenced. (Group 1)\nTableau_Travel_Country_Name: Travel history of the positive patient in the last XX days. This field will be used to label those patients who tested positive and had a travel history according to the IDN and PRP databases. (Group 2)\n\n\n\n\nFirstly, we need to clean the data to ensure that the data is of a good enough quality and no double-counting occurs. In particular we are going to focus on three aspects:\n\nRe-code lineage into relevant variants. In particular:\na. Rename AY.XX into parent lineage Delta B.1.659.2.\nb. Rename all BA.X.X into the parent lineage BA.X (e.g. BA.1.1 is renamed as BA.1).\nc. Rename variant B.1.1.529 as BA.1 as per this description.\nFix records in which age is not defined by retrieving the age from the year of birth from the Emirates ID.\n\n\nRemove invalid records, namely:\na. EIDs that are invalid and do not correspond to any record (5145 records).\nb. Records with lineage labeled as Jul 25 2022 12:00AM (2033 records).\nc. Records with a CollectionDateTime before February 1st, 2020 (6 records).\nRemove duplicate IDs covering the same infection case are present in the database (e.g. two EIDs for the same date labeled the lineage as BA.2 and BA.2.1).\nSelect for the patients sequenced only those whose lineage sequenced have at least more than 500 records.\nCheck also how many records labeled as non-sequenced have a lab assigned according to the dashboard data from ADPHC.\n\nOnce the data is cleaned, we can summarize the number of cases per variant recorded and compare it to the values published by ADPHC.\n\n\nLineages in the dataset appear with multiple names. Some of those lineages were recorded with multiple names. In the original dataset there are 1000 different variants labeled. Some of the reasons for so many sublineages defined registered were\n\nAll sublineages were registered (e.g.: BA.2.40, BA.2.75)\nThe mutation of the sublineage was labeled registered (e.g.: AY.17 (S:D1118Y))\n\nSome sublineages contain a certain description (e.g. BA.2 (Recombinant?))\nSome sublineages are double coded (e.g.: “Delta/B.1.617.2”)\nSome sublineages were wrongly labeled (e.g.: ~2000 records with “Jul 25 2022 12:00AM” as lineage)\n\nTherefore, an aggregation criteria needs to be defined. For the purposes of this report, the following criteria was used:\n\nDescription of mutations were removed (e.g. AY.17 (S:D1118Y) becomes AY.17)\nAll variants were re-defined to their main sub-lineages (e.g.: AY.17 or AY.103 were redefined as B.1.617.2).\nDouble labeling was defined as per the lineage name (e.g.:Delta/B.1.617.2 was coded as B.1.617.2).\nVariant B.1.1.529 was renamed as BA.1.\nAll records with lineage labeled as “Jul 25 2022 12:00AM” were removed.\n\nAfter the re-coding of the lineages in the databases, the number of records for each lineage (for those lineages that contained more than 500 records) is shown in Figure @ref(fig:check-recoded-lineages). It is important to note that this distribution is done with the raw data before removing duplicates in the data.\n\n\n\n\n\nNumber of records per lineage before removal of duplicates.\n\n\n\n\nOnce lineages have been re-coded and the data cleaned from invalid records, we can move into the removal of duplicates.\n\n\n\nAfter recoding lineages and fixing the records that were considered as invalid, the next step consists of cleaning the data by removing all the duplicates.\nFirstly, We check the number of duplicate IDs appearing in the database. This is shown in ?@tbl-duplicate-ids-check:\n\n\n\n\n\n\nPseudoID\n# records\n\n\n\n\n{6BFEAEC8-A945-48B5-B87B-59E8E6F64435}\n340\n\n\n{20A622C3-06CB-4E9D-86E0-0632490450B6}\n144\n\n\n{37D050C7-4927-4AC6-B43E-6631A462EC3D}\n140\n\n\n{B3BA3F02-9D09-42A4-AF4F-8DEC36CE5007}\n140\n\n\n{080D687A-A6E6-4BC1-9098-EE740BEE0EC9}\n120\n\n\n{492A371D-3629-455E-9FE2-8760C6B1783F}\n120\n\n\n{74E26468-4BF9-4503-BAE4-785942560BED}\n120\n\n\n{9DC90E03-1C5A-40F0-8330-3BDD7DE5D57D}\n120\n\n\n{C9EEA6BA-0D32-4BC1-9E4E-5A4E550689DE}\n120\n\n\n{F80A583E-AC85-4C45-9F75-823AD0592E87}\n120\n\n\n\n\n\n\n\nAs we see in the summary table, there are many patients that are repeated in the records (as many as 340 times).\nFor example, we can see the different labels for a given patient in @tab-patient-multiple-records:\n\n\n\n\nDifferent records for the same infection(s) for patient with PseudoID\n\n\nCollectionDateTime\nNationality\nAge\nLineage.Short\nTableau_Travel_Country_Name\nTableau_ICU_Status\nPerforming.Facility\nSequenced\n\n\n\n\n2021-12-25 00:00:00\nExpatraite\n30\n\nYes\nOthers\nDanat Al Emarat Hospital\n0\n\n\n2021-12-25 00:00:00\nExpatraite\n31\n\nYes\nOthers\nDanat Al Emarat Hospital\n0\n\n\n2021-12-25 00:00:00\nMOROCCO\n30\n\nYes\nOthers\nPure Health\n0\n\n\n2021-12-25 00:00:00\nMOROCCO\n30\n\nYes\nOthers\nPure Health\n0\n\n\n2021-12-25 00:00:00\nExpatraite\n31\nBA.1\nSERBIA\nOthers\nDanat Al Emarat Hospital\n1\n\n\n2022-06-12 00:00:00\nMOROCCO\n30\nBA.5\nUNKNOWN\nOthers\nPure Health\n1\n\n\n2021-12-25 00:00:00\nExpatraite\n30\n\nYes\nOthers\nPure Health\n0\n\n\n2021-12-25 00:00:00\nExpatraite\n31\n\nYes\nOthers\nPure Health\n0\n\n\n2021-12-25 00:00:00\nMOROCCO\n31\n\nYes\nOthers\nDanat Al Emarat Hospital\n0\n\n\n2021-12-25 00:00:00\nMOROCCO\n31\n\nYes\nOthers\nDanat Al Emarat Hospital\n0\n\n\n2021-12-28 00:00:00\nMOROCCO\n31\n\nUnknown\n\n\n0\n\n\n2022-06-12 00:00:00\nExpatraite\n30\nBA.5\nUNKNOWN\nOthers\nPure Health\n1\n\n\n2022-06-12 00:00:00\nExpatraite\n30\nBA.5\nUNKNOWN\nOthers\nPure Health\n1\n\n\n2022-06-12 00:00:00\nExpatraite\n30\nBA.5\n\nOthers\nPure Health\n1\n\n\n2022-06-12 00:00:00\nExpatraite\n31\nBA.5\n\nOthers\nPure Health\n1\n\n\n2022-06-12 00:00:00\nMOROCCO\n30\nBA.5\n\nOthers\nPure Health\n1\n\n\n2022-06-12 00:00:00\nMOROCCO\n30\nBA.5\n\nOthers\nPure Health\n1\n\n\n\n\n\n\n\nTherefore it looks like many patients are registered as repeated cases for the same infection episode. Those patients had differences in the label in the following fields:\n\nVariant names (e.g. BA.1 and BA.1.1 for the same patient - not shown in table after recoding.\nCollection dates (e.g. patient collection date was 7/2/2021 and 8/2/2021).\nDifferent values for Tableau_Country_Names (e.g. Yes/No/Country name).\nInfection episodes (e.g. one infection in 30/07/2021 and 01/02/2022).\n\nTherefore, we need an additional aggregation step to identify each infection episode as a unique record. The following steps were followed to fix this issue with duplicate EIDs:\n\nIdentify each episode of infection within 30 days as one infection episode.\nDefine Collection Date as the first date in which the sample of the infected individual was collected\nFill the nationality field with the record that contains a nationality\nFill the lineage field with the first record of the patient that contains a proper lineage\nCreate a Travel.Country field in which to register the country name (if available) for the given ID.\nCreate a Travel.History field in which to register the labels Yes/No/Unknown for the given ID.\nCreate a Traveled field with Yes/No if there is a valid field in Travel.Country or Travel.History (e.g. patient with empty field in Travel.History but with Travel.Country label). This means that those patients with travel history unknown and/or travel country name as unknown were considered as non-travelers.\nRegister the first non-empty field for Tableau_ICU_Status.\nRegister the first non-empty field for Performing Facility for sequenced patients (when available).\n\nAfter cleaning the data, we can check how many times a person is repeated in Table @ref(tab:duplicate-ids-cleaned-check).\n\n\n\n\nNumber of records per ID number of the 10 most repeated records in the database.\n\n\nPseudoID\n# records\n\n\n\n\n{92E11469-6664-4C51-B14B-45C1844C6761}\n6\n\n\n{53E1C60B-BDD6-4A59-85D4-3152AF8FE89F}\n5\n\n\n{763E6F05-FA36-4203-B78B-58DB686BEC1E}\n5\n\n\n{8E907F96-5A60-4B8C-97BD-D22B0B7DFBDE}\n5\n\n\n{A5E35727-E930-44CB-A734-10429FAAB9E1}\n5\n\n\n{CDD7B77C-D1AC-43A8-B257-D1EEF6AC6689}\n5\n\n\n{F719CB95-F15C-4EED-8666-46D2030BA2CE}\n5\n\n\n{0461EE2E-5C5A-4188-9821-F625C174D335}\n4\n\n\n{0506B233-B0B9-4E64-B7BC-F61A7396923C}\n4\n\n\n{112090DF-F669-41D9-BB11-381F86D64F6C}\n4\n\n\n\n\n\n\n\nIn addition, we can check also if the record of the patient shown in Table @ref(tab:patient-multiple-records) has become the expected record in Table @ref(tab:patient-multiple-records-cleaned):\n\n\n\n\nDifferent records for the same infection(s) for patient with PseudoID\n\n\nCollection.Date\nNationality\nAge\nLineage\nTravel.History\nTableau.ICU.Status\nPerforming.Facility\nSequenced\n\n\n\n\n2021-12-25\nMOROCCO\n30\nBA.1\nYes\nOthers\nDanat Al Emarat Hospital\n1\n\n\n2022-06-09\nMOROCCO\n30\nBA.5\nYes\nOthers\nG42 Laboratory\n1\n\n\n\n\n\n\n\nThe distribution of lineages after the removal of duplicate records is shown in Figure @ref(fig:remove-duplicate-records-plot):\n\n\n\n\n\nNumber of people sequenced per variant after data cleaning. Only those variants that were sequenced with more than records per lineage are shown.\n\n\n\n\nWith the lineages unified and the duplicates removed, we can go ahead and study the data.\n\n\n\n\nOne important aspect to compare population sequenced vs. non-sequenced is to see how the compare in terms of age. This will be shown the following subsections\n\n\nOnce we have separated the sequenced with the non-sequenced data we can compare the distribution of cases for the not sequenced people. This will be compared with the sequenced data. This show in Figure @ref(fig:group-by-age-density):\n\n\n\n\n\nDensity distribution of positive cases for sequenced and non-sequenced people\n\n\n\n\nWe can see also the total count for sequenced and non-sequenced groups in Figure @ref(fig:group-by-age-histogram):\n\n\n\n\n\nHistogram fo the distribution of positive cases for sequenced and non-sequenced people\n\n\n\n\n\n\n\nWe can break down the totals shown in Figure @ref(fig:distribution-by-reason-seq) into the age distribution by reason for sequencing:\n\n\n\n\n\nDensity distribution of positive cases per reason of sequencing for sequenced and non-sequenced people\n\n\n\n\n\n\n\nWe can see the distribution of the samples based on their nationality in Figure @ref(fig:distribution-by-nationality):\n\n\n\n\n\nDue to the high number of labels however, it is difficult to define a significant difference for nationality regarding the samples for the sequenced and non-sequenced people.\n\n\n\nAnother aspect that could be studied was the distribution of the samples based on their risk factor. In this case, We can see the distribution of the samples based on their comorbidities in Figures @ref(fig:distribution-by-comorbidities-totals) and @ref(fig:distribution-by-comorbidities):\n\n\n\n\n\n\n\n\n\n\nIn Figure @ref(fig:distribution-by-comorbidities) we can see that the majority of the population had no comorbidities (&gt; 99%).\n\n\n\nAnother layer of granularity is the age distribution per lab. This could be helpful if we are trying to include only labs with an age distribution similar to the non-sequenced people. This is shown in Figure @ref(fig:samples-per-lab):\n\n\n\n\n\nAge distribution of samples collected by each lab.\n\n\n\n\n\n\n\n\nOne more way to look at the data is to see how it is distributed over time, regardless of the age of the population. In particular, we can look at the distribution of cases per lineage sequenced, the hospitalisations and deaths\n\n\nWe can also plot the lineage evolution over time. We can do it per week for each variant count in Figure @ref(fig:lineage-over-time-N):\n\n\n\n\n\nWeekly total count of lineages sequenced. Only those lineages with 500 records or more in the entire dataset are shown.\n\n\n\n\nAnother way to visualize better which variant is taking over and when is to visualize this as a percentage. This is show in Figure #fig-lineage-over-time-perc:\n\n\n\n\n\nWeekly percentage of lineages sequenced. Only those lineages with 500 records or more in the entire dataset are shown.\n\n\n\n\n\n\n\nOne thing we can do is to summarise per month the number of people tested positive, hospitalized and deaths to compare with the sequenced data. The weekly hospitalisations are shown in Figure @ref(fig:biweekly-hospitalisations):\n\n\n\n\n\nBi-weekly percentage of hospitalisation people per sequencing status.\n\n\n\n\nAs shown in Figure #fig-biweekly-hospitalisations, it is clear that the proportion of people hospitalised from sequenced people is higher than those from people who are not sequenced. Therefore, a direct estimation from the sequenced data only will estimate a higher hospitalisation rate for each variant studied and therefore it is required to adjust for that bias.\n\n\n\nThe number of deaths per sequencing status is shown in Figure @ref(fig:biweekly-deaths):\n\n\n\n\n\nBi-weekly percentage of deaths by sequenced status."
  },
  {
    "objectID": "posts/data-analysis-post/index.html#sampling-issues",
    "href": "posts/data-analysis-post/index.html#sampling-issues",
    "title": "Data analysis to evaluate the hospitalisation risk between SARS-CoV-2 Variants",
    "section": "",
    "text": "The sampling strategy followed in the United Arab Emirates from January 1st, 2022 comprised the sequencing of samples from individuals that belonged to one of the following groups:\n\nAll people in ICU/HDU and/or deceased.\nAll people who entered through a port of entry and had a positive PCR.\nThe rest of the infected people were sequenced based on a random sample proportionally to the number of positives reported by each lab on a weekly basis.\n\nSampling prior to the stated day needs to be double-checked for proper interpretation.\nThe sampling strategy described follows WHO recommendation. Therefore, it might be of interest to develop a methodology in which an unbiased (or systematically low bias) estimator of the sample is found after following WHO recommendations for sampling COVID-19 patients.\nFirst, we are going to load the dataset and examine it:\n\n\n'data.frame':   2535161 obs. of  34 variables:\n $ PseudoID                   : chr  \"{FA0C53C0-82F7-4803-AF51-F555BA559850}\" \"{FF55A071-A266-4A57-BE88-F555BA825A10}\" \"{973E11D1-51F1-4069-96D7-F55625907981}\" \"{B37D192F-AA2C-4812-890B-F5573B6873D2}\" ...\n $ Lineage                    : chr  \"B.1.617.2\" \"\" \"B.1.617.2\" \"\" ...\n $ CollectionDateTime         : chr  \"2021-11-24 00:00:00\" \"2022-08-08 00:00:00\" \"2021-06-09 00:00:00\" \"2022-06-08 00:00:00\" ...\n $ Dose1_Vaccine_Type         : chr  \"Pfizer\" \"Pfizer\" \"\" \"Sinopharm\" ...\n $ Dose1_VaccinationDate      : chr  \"2022-03-13\" \"2021-09-15\" \"\" \"2021-05-02\" ...\n $ Dose2_Vaccine_Type         : chr  \"\" \"Pfizer\" \"\" \"Sinopharm\" ...\n $ Dose2_VaccinationDate      : chr  \"\" \"2021-10-06\" \"\" \"2021-05-23\" ...\n $ Dose3_Vaccine_Type         : chr  \"\" \"\" \"\" \"Sinopharm\" ...\n $ Dose3_VaccinationDate      : chr  \"\" \"\" \"\" \"2021-12-21\" ...\n $ Dose4_Vaccine_Type         : chr  \"\" \"\" \"\" \"\" ...\n $ Dose4_VaccinationDate      : chr  \"\" \"\" \"\" \"\" ...\n $ VaccinationStatus          : chr  \"NotVaccincated\" \"Full\" \"NotVaccincated\" \"Full\" ...\n $ NoOfDoses                  : int  1 2 0 3 0 3 3 0 3 3 ...\n $ LineageHospitalized        : int  0 1 0 0 0 0 0 0 0 0 ...\n $ IsDied                     : int  0 0 0 0 0 0 0 0 0 0 ...\n $ Death_Date                 : chr  \"\" \"\" \"\" \"\" ...\n $ LOS                        : int  NA 12 2 NA NA NA NA NA NA NA ...\n $ hospital_discharge_date    : chr  \"\" \"2022-08-22 11:59:00\" \"2022-12-21 13:28:00\" \"\" ...\n $ Date_hospitalization       : chr  \"\" \"2022-08-10 18:11:00.000\" \"2022-12-19 09:23:43.000\" \"\" ...\n $ Covid_Variant              : chr  \"Delta\" \"\" \"Delta\" \"\" ...\n $ Gender                     : chr  \"Male\" \"Male\" \"Female\" \"Male\" ...\n $ Nationality                : chr  \"Expatraite\" \"PHILIPPINES\" \"UNITED ARAB EMIRATES\" \"UNITED ARAB EMIRATES\" ...\n $ LineageRecorded            : chr  \"\" \"\" \"\" \"\" ...\n $ Comorbities                : int  0 0 0 0 0 0 0 0 0 0 ...\n $ PriorInfection             : int  NA NA 1 NA NA NA NA NA 1 1 ...\n $ PriorInfection_Count       : int  NA NA 4 NA NA NA NA NA 2 2 ...\n $ Band15Years                : chr  \"15-29\" \"30-44\" \"60+\" \"45-59\" ...\n $ Tableau_ICU_Status         : chr  \"Others\" \"\" \"Others\" \"\" ...\n $ Tableau_Travel_Country_Name: chr  \"\" \"No\" \"UNKNOWN\" \"No\" ...\n $ Performing.Facility        : chr  \"TW Tawam Hospital\" \"Mediclinic\" \"\" \"\" ...\n $ Sequenced                  : chr  \"1\" \"0\" \"1\" \"0\" ...\n $ AGE                        : chr  \"21\" \"33\" \"87\" \"49\" ...\n $ CASES                      : chr  \"146\" \"1475\" \"4254\" \"4052\" ...\n $ TESTED                     : int  182946 173947 144956 165451 178620 277402 260176 123265 298367 248765 ...\n\n\nAs we see, we have information per patient for the following variables:\n\nPseudoID: Unique identifier for each patient.\nLineage: Identification of the lineage sequenced.\nCollectionDateTime: Date and time in which the PCR sample was collected for the infected patient.\nDose_VaccinationDate: Date in which the vaccination of the dose referred was taken.\nDose_Vaccine_Type: Type of vaccine that was taken.\nVaccination Status: Fully vaccinated if the individual got at least two doses or unvaccinated.\nNoOfDoses: Number of vaccine doses received per individual.\nLineageHospitalized: Identification of hospitalization for the patient who was sequenced.\nIsDied: Identification of death for the patient.\nDeath_Date: Date in which the patient died.\nLOS: Length of stay in the hospital.\nHospital_Discharge_date: Date in which the patient was discharged.\nDate_hospitalization: Date in which the patient was admitted.\nGender: Male or Female.\nNationality: Expatriate or Local.\nCovid_Variant: Recoding of the variant of the COVID Lineage.\nComorbidities: Number of comorbidities per patient.\nPriorInfection: Flag if the patient registered a prior infection before the current positive.\nPriorInfection_Count: Number of previous infections registered.\nAge: Age of the patient.\nBand15Years: Age group of 15 years old for the patient.\nSequenced: Identification for patients that were sequenced or not sequenced.\nPerforming.Facility: Lab conducting the collection of the sample that was sequencing. This field will be used to estimate the population properties of those that were randomly sequenced. In particular, checking the randomness assumption is a goal of the work presented in this document. (Group 3)\nTableau_ICU_Status: Label if the patient is ICU/HDU or dead. This field will be used to label the patients as HDU/ICU pr dead as reason per sequencing. All patients labeled in this field were sequenced. (Group 1)\nTableau_Travel_Country_Name: Travel history of the positive patient in the last XX days. This field will be used to label those patients who tested positive and had a travel history according to the IDN and PRP databases. (Group 2)"
  },
  {
    "objectID": "posts/data-analysis-post/index.html#cleaning-data-procedure",
    "href": "posts/data-analysis-post/index.html#cleaning-data-procedure",
    "title": "Data analysis to evaluate the hospitalisation risk between SARS-CoV-2 Variants",
    "section": "",
    "text": "Firstly, we need to clean the data to ensure that the data is of a good enough quality and no double-counting occurs. In particular we are going to focus on three aspects:\n\nRe-code lineage into relevant variants. In particular:\na. Rename AY.XX into parent lineage Delta B.1.659.2.\nb. Rename all BA.X.X into the parent lineage BA.X (e.g. BA.1.1 is renamed as BA.1).\nc. Rename variant B.1.1.529 as BA.1 as per this description.\nFix records in which age is not defined by retrieving the age from the year of birth from the Emirates ID.\n\n\nRemove invalid records, namely:\na. EIDs that are invalid and do not correspond to any record (5145 records).\nb. Records with lineage labeled as Jul 25 2022 12:00AM (2033 records).\nc. Records with a CollectionDateTime before February 1st, 2020 (6 records).\nRemove duplicate IDs covering the same infection case are present in the database (e.g. two EIDs for the same date labeled the lineage as BA.2 and BA.2.1).\nSelect for the patients sequenced only those whose lineage sequenced have at least more than 500 records.\nCheck also how many records labeled as non-sequenced have a lab assigned according to the dashboard data from ADPHC.\n\nOnce the data is cleaned, we can summarize the number of cases per variant recorded and compare it to the values published by ADPHC.\n\n\nLineages in the dataset appear with multiple names. Some of those lineages were recorded with multiple names. In the original dataset there are 1000 different variants labeled. Some of the reasons for so many sublineages defined registered were\n\nAll sublineages were registered (e.g.: BA.2.40, BA.2.75)\nThe mutation of the sublineage was labeled registered (e.g.: AY.17 (S:D1118Y))\n\nSome sublineages contain a certain description (e.g. BA.2 (Recombinant?))\nSome sublineages are double coded (e.g.: “Delta/B.1.617.2”)\nSome sublineages were wrongly labeled (e.g.: ~2000 records with “Jul 25 2022 12:00AM” as lineage)\n\nTherefore, an aggregation criteria needs to be defined. For the purposes of this report, the following criteria was used:\n\nDescription of mutations were removed (e.g. AY.17 (S:D1118Y) becomes AY.17)\nAll variants were re-defined to their main sub-lineages (e.g.: AY.17 or AY.103 were redefined as B.1.617.2).\nDouble labeling was defined as per the lineage name (e.g.:Delta/B.1.617.2 was coded as B.1.617.2).\nVariant B.1.1.529 was renamed as BA.1.\nAll records with lineage labeled as “Jul 25 2022 12:00AM” were removed.\n\nAfter the re-coding of the lineages in the databases, the number of records for each lineage (for those lineages that contained more than 500 records) is shown in Figure @ref(fig:check-recoded-lineages). It is important to note that this distribution is done with the raw data before removing duplicates in the data.\n\n\n\n\n\nNumber of records per lineage before removal of duplicates.\n\n\n\n\nOnce lineages have been re-coded and the data cleaned from invalid records, we can move into the removal of duplicates.\n\n\n\nAfter recoding lineages and fixing the records that were considered as invalid, the next step consists of cleaning the data by removing all the duplicates.\nFirstly, We check the number of duplicate IDs appearing in the database. This is shown in ?@tbl-duplicate-ids-check:\n\n\n\n\n\n\nPseudoID\n# records\n\n\n\n\n{6BFEAEC8-A945-48B5-B87B-59E8E6F64435}\n340\n\n\n{20A622C3-06CB-4E9D-86E0-0632490450B6}\n144\n\n\n{37D050C7-4927-4AC6-B43E-6631A462EC3D}\n140\n\n\n{B3BA3F02-9D09-42A4-AF4F-8DEC36CE5007}\n140\n\n\n{080D687A-A6E6-4BC1-9098-EE740BEE0EC9}\n120\n\n\n{492A371D-3629-455E-9FE2-8760C6B1783F}\n120\n\n\n{74E26468-4BF9-4503-BAE4-785942560BED}\n120\n\n\n{9DC90E03-1C5A-40F0-8330-3BDD7DE5D57D}\n120\n\n\n{C9EEA6BA-0D32-4BC1-9E4E-5A4E550689DE}\n120\n\n\n{F80A583E-AC85-4C45-9F75-823AD0592E87}\n120\n\n\n\n\n\n\n\nAs we see in the summary table, there are many patients that are repeated in the records (as many as 340 times).\nFor example, we can see the different labels for a given patient in @tab-patient-multiple-records:\n\n\n\n\nDifferent records for the same infection(s) for patient with PseudoID\n\n\nCollectionDateTime\nNationality\nAge\nLineage.Short\nTableau_Travel_Country_Name\nTableau_ICU_Status\nPerforming.Facility\nSequenced\n\n\n\n\n2021-12-25 00:00:00\nExpatraite\n30\n\nYes\nOthers\nDanat Al Emarat Hospital\n0\n\n\n2021-12-25 00:00:00\nExpatraite\n31\n\nYes\nOthers\nDanat Al Emarat Hospital\n0\n\n\n2021-12-25 00:00:00\nMOROCCO\n30\n\nYes\nOthers\nPure Health\n0\n\n\n2021-12-25 00:00:00\nMOROCCO\n30\n\nYes\nOthers\nPure Health\n0\n\n\n2021-12-25 00:00:00\nExpatraite\n31\nBA.1\nSERBIA\nOthers\nDanat Al Emarat Hospital\n1\n\n\n2022-06-12 00:00:00\nMOROCCO\n30\nBA.5\nUNKNOWN\nOthers\nPure Health\n1\n\n\n2021-12-25 00:00:00\nExpatraite\n30\n\nYes\nOthers\nPure Health\n0\n\n\n2021-12-25 00:00:00\nExpatraite\n31\n\nYes\nOthers\nPure Health\n0\n\n\n2021-12-25 00:00:00\nMOROCCO\n31\n\nYes\nOthers\nDanat Al Emarat Hospital\n0\n\n\n2021-12-25 00:00:00\nMOROCCO\n31\n\nYes\nOthers\nDanat Al Emarat Hospital\n0\n\n\n2021-12-28 00:00:00\nMOROCCO\n31\n\nUnknown\n\n\n0\n\n\n2022-06-12 00:00:00\nExpatraite\n30\nBA.5\nUNKNOWN\nOthers\nPure Health\n1\n\n\n2022-06-12 00:00:00\nExpatraite\n30\nBA.5\nUNKNOWN\nOthers\nPure Health\n1\n\n\n2022-06-12 00:00:00\nExpatraite\n30\nBA.5\n\nOthers\nPure Health\n1\n\n\n2022-06-12 00:00:00\nExpatraite\n31\nBA.5\n\nOthers\nPure Health\n1\n\n\n2022-06-12 00:00:00\nMOROCCO\n30\nBA.5\n\nOthers\nPure Health\n1\n\n\n2022-06-12 00:00:00\nMOROCCO\n30\nBA.5\n\nOthers\nPure Health\n1\n\n\n\n\n\n\n\nTherefore it looks like many patients are registered as repeated cases for the same infection episode. Those patients had differences in the label in the following fields:\n\nVariant names (e.g. BA.1 and BA.1.1 for the same patient - not shown in table after recoding.\nCollection dates (e.g. patient collection date was 7/2/2021 and 8/2/2021).\nDifferent values for Tableau_Country_Names (e.g. Yes/No/Country name).\nInfection episodes (e.g. one infection in 30/07/2021 and 01/02/2022).\n\nTherefore, we need an additional aggregation step to identify each infection episode as a unique record. The following steps were followed to fix this issue with duplicate EIDs:\n\nIdentify each episode of infection within 30 days as one infection episode.\nDefine Collection Date as the first date in which the sample of the infected individual was collected\nFill the nationality field with the record that contains a nationality\nFill the lineage field with the first record of the patient that contains a proper lineage\nCreate a Travel.Country field in which to register the country name (if available) for the given ID.\nCreate a Travel.History field in which to register the labels Yes/No/Unknown for the given ID.\nCreate a Traveled field with Yes/No if there is a valid field in Travel.Country or Travel.History (e.g. patient with empty field in Travel.History but with Travel.Country label). This means that those patients with travel history unknown and/or travel country name as unknown were considered as non-travelers.\nRegister the first non-empty field for Tableau_ICU_Status.\nRegister the first non-empty field for Performing Facility for sequenced patients (when available).\n\nAfter cleaning the data, we can check how many times a person is repeated in Table @ref(tab:duplicate-ids-cleaned-check).\n\n\n\n\nNumber of records per ID number of the 10 most repeated records in the database.\n\n\nPseudoID\n# records\n\n\n\n\n{92E11469-6664-4C51-B14B-45C1844C6761}\n6\n\n\n{53E1C60B-BDD6-4A59-85D4-3152AF8FE89F}\n5\n\n\n{763E6F05-FA36-4203-B78B-58DB686BEC1E}\n5\n\n\n{8E907F96-5A60-4B8C-97BD-D22B0B7DFBDE}\n5\n\n\n{A5E35727-E930-44CB-A734-10429FAAB9E1}\n5\n\n\n{CDD7B77C-D1AC-43A8-B257-D1EEF6AC6689}\n5\n\n\n{F719CB95-F15C-4EED-8666-46D2030BA2CE}\n5\n\n\n{0461EE2E-5C5A-4188-9821-F625C174D335}\n4\n\n\n{0506B233-B0B9-4E64-B7BC-F61A7396923C}\n4\n\n\n{112090DF-F669-41D9-BB11-381F86D64F6C}\n4\n\n\n\n\n\n\n\nIn addition, we can check also if the record of the patient shown in Table @ref(tab:patient-multiple-records) has become the expected record in Table @ref(tab:patient-multiple-records-cleaned):\n\n\n\n\nDifferent records for the same infection(s) for patient with PseudoID\n\n\nCollection.Date\nNationality\nAge\nLineage\nTravel.History\nTableau.ICU.Status\nPerforming.Facility\nSequenced\n\n\n\n\n2021-12-25\nMOROCCO\n30\nBA.1\nYes\nOthers\nDanat Al Emarat Hospital\n1\n\n\n2022-06-09\nMOROCCO\n30\nBA.5\nYes\nOthers\nG42 Laboratory\n1\n\n\n\n\n\n\n\nThe distribution of lineages after the removal of duplicate records is shown in Figure @ref(fig:remove-duplicate-records-plot):\n\n\n\n\n\nNumber of people sequenced per variant after data cleaning. Only those variants that were sequenced with more than records per lineage are shown.\n\n\n\n\nWith the lineages unified and the duplicates removed, we can go ahead and study the data."
  },
  {
    "objectID": "posts/data-analysis-post/index.html#comparison-between-sequenced-and-non-sequenced-samples",
    "href": "posts/data-analysis-post/index.html#comparison-between-sequenced-and-non-sequenced-samples",
    "title": "Data analysis to evaluate the hospitalisation risk between SARS-CoV-2 Variants",
    "section": "",
    "text": "One important aspect to compare population sequenced vs. non-sequenced is to see how the compare in terms of age. This will be shown the following subsections\n\n\nOnce we have separated the sequenced with the non-sequenced data we can compare the distribution of cases for the not sequenced people. This will be compared with the sequenced data. This show in Figure @ref(fig:group-by-age-density):\n\n\n\n\n\nDensity distribution of positive cases for sequenced and non-sequenced people\n\n\n\n\nWe can see also the total count for sequenced and non-sequenced groups in Figure @ref(fig:group-by-age-histogram):\n\n\n\n\n\nHistogram fo the distribution of positive cases for sequenced and non-sequenced people\n\n\n\n\n\n\n\nWe can break down the totals shown in Figure @ref(fig:distribution-by-reason-seq) into the age distribution by reason for sequencing:\n\n\n\n\n\nDensity distribution of positive cases per reason of sequencing for sequenced and non-sequenced people\n\n\n\n\n\n\n\nWe can see the distribution of the samples based on their nationality in Figure @ref(fig:distribution-by-nationality):\n\n\n\n\n\nDue to the high number of labels however, it is difficult to define a significant difference for nationality regarding the samples for the sequenced and non-sequenced people.\n\n\n\nAnother aspect that could be studied was the distribution of the samples based on their risk factor. In this case, We can see the distribution of the samples based on their comorbidities in Figures @ref(fig:distribution-by-comorbidities-totals) and @ref(fig:distribution-by-comorbidities):\n\n\n\n\n\n\n\n\n\n\nIn Figure @ref(fig:distribution-by-comorbidities) we can see that the majority of the population had no comorbidities (&gt; 99%).\n\n\n\nAnother layer of granularity is the age distribution per lab. This could be helpful if we are trying to include only labs with an age distribution similar to the non-sequenced people. This is shown in Figure @ref(fig:samples-per-lab):\n\n\n\n\n\nAge distribution of samples collected by each lab."
  },
  {
    "objectID": "posts/data-analysis-post/index.html#distribution-of-samples-over-time",
    "href": "posts/data-analysis-post/index.html#distribution-of-samples-over-time",
    "title": "Data analysis to evaluate the hospitalisation risk between SARS-CoV-2 Variants",
    "section": "",
    "text": "One more way to look at the data is to see how it is distributed over time, regardless of the age of the population. In particular, we can look at the distribution of cases per lineage sequenced, the hospitalisations and deaths\n\n\nWe can also plot the lineage evolution over time. We can do it per week for each variant count in Figure @ref(fig:lineage-over-time-N):\n\n\n\n\n\nWeekly total count of lineages sequenced. Only those lineages with 500 records or more in the entire dataset are shown.\n\n\n\n\nAnother way to visualize better which variant is taking over and when is to visualize this as a percentage. This is show in Figure #fig-lineage-over-time-perc:\n\n\n\n\n\nWeekly percentage of lineages sequenced. Only those lineages with 500 records or more in the entire dataset are shown.\n\n\n\n\n\n\n\nOne thing we can do is to summarise per month the number of people tested positive, hospitalized and deaths to compare with the sequenced data. The weekly hospitalisations are shown in Figure @ref(fig:biweekly-hospitalisations):\n\n\n\n\n\nBi-weekly percentage of hospitalisation people per sequencing status.\n\n\n\n\nAs shown in Figure #fig-biweekly-hospitalisations, it is clear that the proportion of people hospitalised from sequenced people is higher than those from people who are not sequenced. Therefore, a direct estimation from the sequenced data only will estimate a higher hospitalisation rate for each variant studied and therefore it is required to adjust for that bias.\n\n\n\nThe number of deaths per sequencing status is shown in Figure @ref(fig:biweekly-deaths):\n\n\n\n\n\nBi-weekly percentage of deaths by sequenced status."
  },
  {
    "objectID": "posts/online-simulator-covid19/index.html",
    "href": "posts/online-simulator-covid19/index.html",
    "title": "COVID-19 online simulator",
    "section": "",
    "text": "This is a post to explain what is done in the online dashboard to simulate the evolution of COVID-19.\nThis code was developed in R and the dashboard was created using Shiny.\nThe online simulation tool is available in this website\n\n\n\nFigure 1: Template of the online model simulator for COVID-19\n\n\nThe model simulator can be seen in Figure 1.\nGet text from here(https://www.ku.ac.ae/khalifa-university-researchers-develop-mathematical-model-to-help-policymakers-and-non-experts-tackle-covid-19-challenges) to adapt it"
  },
  {
    "objectID": "posts/quantification-yields-atp/index.html",
    "href": "posts/quantification-yields-atp/index.html",
    "title": "Pathway evaluation for the quantification of ATP yields",
    "section": "",
    "text": "Microbial path\nThis is a post to explain what was done in the framework to calculate the yields of ATP using first principles.\nThis code was developed in MATLAB together with its visualizations.\n\n\n\nFigure 1: Template of the dashboard including the online model simulator for a series of bioreactors\n\n\nThe model simulator can be seen in Figure 1.\nAn interest insight is to plot the methanogenic niche, also known as the range in which both reactions appear to be feasible. The methanogenic niche can be seen in Figure 2.\n\n\n\nFigure 2: Methanogenic niche\n\n\nDecoding the Energy Dance of Microbes: Unraveling ATP Production in Wastewater Treatment\nImagine a bustling microbial nightclub deep within a sewage treatment plant. The VIP guests? Tiny organisms that break down waste, turning it into something less gross. But how do they do it? And which pathways lead to the best dance moves (read: ATP production)?\nIn a recent study, scientists put on their lab coats and hit the microbial dance floor. Their mission: to figure out which biochemical pathways yield the most ATP (the cellular energy currency) during propionate oxidation. Let’s break it down:\nThe Pathway Shuffle: Microbes have options. Different pathways involve various metabolites, electron carriers, and ATP steps. Picture a dance-off: Some pathways moonwalk, while others cha-cha with protons (or their equivalents). The Feasibility Check: To get past the bouncer (i.e., feasibility evaluation), a pathway must meet two criteria: All reaction steps must have nonpositive Gibbs energy changes (no energy vampires allowed!). Metabolite concentrations must stay within a physiological range (10⁻⁶ to 10⁻² M). The ATP Champions: Under optimal conditions, the Smithella pathway struts its stuff, yielding the most ATP. The methylmalonyl-coenzyme A (CoA) pathways? They’re like backup dancers—only grooving with pyruvate in a cyclical routine. The Methane Afterparty: In methanogenic environments (think swamps), other pathways take the spotlight. Lactate and hydroxypropionyl-CoA pathways? They’re the headliners, belting out ATP hits. Electron Transfer Tango: How do microbes exchange electrons? Some flirt with dissolved hydrogen via the Smithella pathway. Others? They salsa with formate or engage in direct electron transfer (fancy footwork!)."
  },
  {
    "objectID": "posts/quantification-yields-atp/index.html#summary",
    "href": "posts/quantification-yields-atp/index.html#summary",
    "title": "Pathway evaluation for the quantification of ATP yields",
    "section": "",
    "text": "Microbial path\nThis is a post to explain what was done in the framework to calculate the yields of ATP using first principles.\nThis code was developed in MATLAB together with its visualizations.\n\n\n\nFigure 1: Template of the dashboard including the online model simulator for a series of bioreactors\n\n\nThe model simulator can be seen in Figure 1.\nAn interest insight is to plot the methanogenic niche, also known as the range in which both reactions appear to be feasible. The methanogenic niche can be seen in Figure 2.\n\n\n\nFigure 2: Methanogenic niche\n\n\nDecoding the Energy Dance of Microbes: Unraveling ATP Production in Wastewater Treatment\nImagine a bustling microbial nightclub deep within a sewage treatment plant. The VIP guests? Tiny organisms that break down waste, turning it into something less gross. But how do they do it? And which pathways lead to the best dance moves (read: ATP production)?\nIn a recent study, scientists put on their lab coats and hit the microbial dance floor. Their mission: to figure out which biochemical pathways yield the most ATP (the cellular energy currency) during propionate oxidation. Let’s break it down:\nThe Pathway Shuffle: Microbes have options. Different pathways involve various metabolites, electron carriers, and ATP steps. Picture a dance-off: Some pathways moonwalk, while others cha-cha with protons (or their equivalents). The Feasibility Check: To get past the bouncer (i.e., feasibility evaluation), a pathway must meet two criteria: All reaction steps must have nonpositive Gibbs energy changes (no energy vampires allowed!). Metabolite concentrations must stay within a physiological range (10⁻⁶ to 10⁻² M). The ATP Champions: Under optimal conditions, the Smithella pathway struts its stuff, yielding the most ATP. The methylmalonyl-coenzyme A (CoA) pathways? They’re like backup dancers—only grooving with pyruvate in a cyclical routine. The Methane Afterparty: In methanogenic environments (think swamps), other pathways take the spotlight. Lactate and hydroxypropionyl-CoA pathways? They’re the headliners, belting out ATP hits. Electron Transfer Tango: How do microbes exchange electrons? Some flirt with dissolved hydrogen via the Smithella pathway. Others? They salsa with formate or engage in direct electron transfer (fancy footwork!)."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Identify age-related conditions Competition\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBioreactor online simulator dashboard\n\n\n\nbioprocesses\n\n\ncode\n\n\nshiny\n\n\n\n\n\n\n\nMauricio Patón\n\n\nFeb 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData analysis to evaluate the hospitalisation risk between SARS-CoV-2 Variants\n\n\n\ncode\n\n\nanalysis\n\n\ncovid19\n\n\n\n\n\n\n\nMauricio Patón\n\n\nJan 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nICR Age-Related Conditions Kaggle Competition\n\n\n\nmachine learning\n\n\ncode\n\n\nshiny\n\n\n\n\n\n\n\nMauricio Patón\n\n\nMar 10, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCOVID-19 Expo Model\n\n\n\ncovid19\n\n\ncode\n\n\nshiny\n\n\n\n\n\n\n\nMauricio Patón\n\n\nJan 3, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCOVID-19 Vaccine allocator\n\n\n\ncovid19\n\n\ncode\n\n\nMATLAB\n\n\n\n\n\n\n\nMauricio Patón\n\n\nOct 3, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCOVID-19 online simulator\n\n\n\ncovid19\n\n\ncode\n\n\nshiny\n\n\n\n\n\n\n\nMauricio Patón\n\n\nMay 3, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPathway evaluation for the quantification of ATP yields\n\n\n\nbioprocesses\n\n\ncode\n\n\nmatlab\n\n\natp yields\n\n\n\n\n\n\n\nMauricio Patón\n\n\nFeb 1, 2020\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mauricio Patón",
    "section": "",
    "text": "Hello there! I am Mauricio, a researcher with over 9 years of experience working with mathematical models.\nI am always looking forward to learn something new. Currently I am expanding my skills to learn how to deploy Machine Learning models in a production environment. I truly look forward to deploy models capable of helping in real-world problems."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Mauricio Patón",
    "section": "",
    "text": "Hi, I am Mauricio. I am an Researcher with over 9 years of experience working with mathematical models. For that that wished to turn into a Data Scientist. Why? I have a passion for modelling and I also want that the models I build are useful. I think then that Machine Learning models are the perfect tool to answer that wish?"
  }
]